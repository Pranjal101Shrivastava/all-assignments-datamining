{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "T_WGLHnUCkNP",
        "outputId": "05a4052c-fdb6-40f2-df4a-94fa76530a22"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/1.8 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.3/1.8 MB\u001b[0m \u001b[31m8.7 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m1.8/1.8 MB\u001b[0m \u001b[31m26.4 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.8/1.8 MB\u001b[0m \u001b[31m19.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/1.2 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m41.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "ipython 7.34.0 requires jedi>=0.16, which is not installed.\u001b[0m\u001b[31m\n",
            "\u001b[0m  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for apache-beam (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for crcmod (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for dill (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for hdfs (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for docopt (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "imbalanced-learn 0.14.0 requires scikit-learn<2,>=1.4.2, but you have scikit-learn 1.3.2 which is incompatible.\n",
            "jaxlib 0.7.2 requires numpy>=2.0, but you have numpy 1.26.4 which is incompatible.\n",
            "distributed 2025.5.0 requires cloudpickle>=3.0.0, but you have cloudpickle 2.2.1 which is incompatible.\n",
            "bigframes 2.26.0 requires pyarrow>=15.0.2, but you have pyarrow 14.0.2 which is incompatible.\n",
            "opencv-contrib-python 4.12.0.88 requires numpy<2.3.0,>=2; python_version >= \"3.9\", but you have numpy 1.26.4 which is incompatible.\n",
            "dask 2025.5.0 requires cloudpickle>=3.0.0, but you have cloudpickle 2.2.1 which is incompatible.\n",
            "jax 0.7.2 requires numpy>=2.0, but you have numpy 1.26.4 which is incompatible.\n",
            "ydf 0.13.0 requires protobuf<7.0.0,>=5.29.1, but you have protobuf 4.25.8 which is incompatible.\n",
            "opencv-python-headless 4.12.0.88 requires numpy<2.3.0,>=2; python_version >= \"3.9\", but you have numpy 1.26.4 which is incompatible.\n",
            "opencv-python 4.12.0.88 requires numpy<2.3.0,>=2; python_version >= \"3.9\", but you have numpy 1.26.4 which is incompatible.\n",
            "umap-learn 0.5.9.post2 requires scikit-learn>=1.6, but you have scikit-learn 1.3.2 which is incompatible.\n",
            "pytensor 2.35.1 requires numpy>=2.0, but you have numpy 1.26.4 which is incompatible.\n",
            "grpcio-status 1.71.2 requires protobuf<6.0dev,>=5.26.1, but you have protobuf 4.25.8 which is incompatible.\n",
            "thinc 8.3.6 requires numpy<3.0.0,>=2.0.0, but you have numpy 1.26.4 which is incompatible.\n",
            "opentelemetry-proto 1.37.0 requires protobuf<7.0,>=5.0, but you have protobuf 4.25.8 which is incompatible.\n",
            "multiprocess 0.70.16 requires dill>=0.3.8, but you have dill 0.3.1.1 which is incompatible.\n",
            "datasets 4.0.0 requires pyarrow>=15.0.0, but you have pyarrow 14.0.2 which is incompatible.\n",
            "cuml-cu12 25.6.0 requires scikit-learn>=1.5, but you have scikit-learn 1.3.2 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ],
      "source": [
        "#cell1\n",
        "# === Install (robust & light) ===\n",
        "!pip -q install -U pip setuptools wheel\n",
        "!pip -q install \"apache-beam==2.56.0\" \"scikit-learn==1.3.2\"\n",
        "# Fallback (uncomment if needed):\n",
        "# !pip -q install \"apache-beam==2.53.0\" \"scikit-learn==1.2.2\"\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# cell 2\n",
        "# === Environment check ===\n",
        "import sys, numpy as np, pandas as pd, apache_beam as beam, sklearn\n",
        "from datetime import datetime\n",
        "print(\"Python:\", sys.version.split()[0])\n",
        "print(\"NumPy:\", np.__version__)\n",
        "print(\"Pandas:\", pd.__version__)\n",
        "print(\"Apache Beam:\", beam.__version__)\n",
        "print(\"scikit-learn:\", sklearn.__version__)\n",
        "print(\"Time:\", datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\"))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C6BepekJbmqm",
        "outputId": "68cd5fd9-1e40-404b-ee7b-a61b9f7fe823"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/apache_beam/__init__.py:74: UserWarning: This version of Apache Beam has not been sufficiently tested on Python 3.12. You may encounter bugs or missing features.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Python: 3.12.12\n",
            "NumPy: 1.26.4\n",
            "Pandas: 2.2.2\n",
            "Apache Beam: 2.56.0\n",
            "scikit-learn: 1.3.2\n",
            "Time: 2025-10-27 02:10:58\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# cell 3\n",
        "# === Create small CSV input ===\n",
        "import os, json\n",
        "from pathlib import Path\n",
        "\n",
        "DATA_DIR = Path(\"/content/data\"); DATA_DIR.mkdir(parents=True, exist_ok=True)\n",
        "OUT_DIR = Path(\"/content/output\"); OUT_DIR.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "rows = [\n",
        "    (\"2025-01-01T00:00:10\",\"u1\",12.5,\"US\",0),\n",
        "    (\"2025-01-01T00:01:25\",\"u2\",300.0,\"US\",1),\n",
        "    (\"2025-01-01T00:01:55\",\"u3\",50.0,\"CA\",0),\n",
        "    (\"2025-01-01T00:02:05\",\"u4\",700.0,\"US\",1),\n",
        "    (\"2025-01-01T00:04:40\",\"u5\",25.0,\"GB\",0),\n",
        "    (\"2025-01-01T00:05:05\",\"u1\",900.0,\"US\",1),\n",
        "    (\"2025-01-01T00:06:10\",\"u2\",10.0,\"CA\",0),\n",
        "    (\"2025-01-01T00:06:59\",\"u3\",75.5,\"US\",0),\n",
        "    (\"2025-01-01T00:09:20\",\"u4\",15.0,\"US\",0),\n",
        "]\n",
        "csv_path = \"/content/data/input_transactions.csv\"\n",
        "with open(csv_path, \"w\") as f:\n",
        "    f.write(\"ts,user,amount,country,is_fraud\\n\")\n",
        "    for r in rows:\n",
        "        f.write(f\"{r[0]},{r[1]},{r[2]},{r[3]},{r[4]}\\n\")\n",
        "\n",
        "!head -n 5 /content/data/input_transactions.csv\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nKGEVHGlcUOG",
        "outputId": "e235baf3-f910-436e-b147-3978223201ac"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ts,user,amount,country,is_fraud\n",
            "2025-01-01T00:00:10,u1,12.5,US,0\n",
            "2025-01-01T00:01:25,u2,300.0,US,1\n",
            "2025-01-01T00:01:55,u3,50.0,CA,0\n",
            "2025-01-01T00:02:05,u4,700.0,US,1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# cell 4\n",
        "# === Helpers: parse, timestamp, DoFn, Composite ===\n",
        "import datetime as dt\n",
        "import apache_beam as beam\n",
        "from apache_beam.io import ReadFromText, WriteToText\n",
        "from apache_beam.options.pipeline_options import PipelineOptions\n",
        "from apache_beam.transforms.window import FixedWindows, TimestampedValue\n",
        "\n",
        "def parse_csv(line: str):\n",
        "    if line.startswith(\"ts,\"):\n",
        "        return None\n",
        "    ts, user, amount, country, is_fraud = line.split(\",\")\n",
        "    return {\"ts\": ts, \"user\": user, \"amount\": float(amount), \"country\": country, \"is_fraud\": int(is_fraud)}\n",
        "\n",
        "def to_timestamped(e: dict):\n",
        "    ts = dt.datetime.fromisoformat(e[\"ts\"])\n",
        "    return TimestampedValue(e, ts.timestamp())\n",
        "\n",
        "class AddRiskScore(beam.DoFn):\n",
        "    def process(self, e):\n",
        "        risk = 0.0\n",
        "        if e[\"amount\"] >= 500: risk += 0.5\n",
        "        if e[\"country\"] == \"US\": risk += 0.2\n",
        "        if e[\"is_fraud\"] == 1: risk += 0.3\n",
        "        e[\"risk\"] = round(min(risk, 1.0), 3)\n",
        "        yield e\n",
        "\n",
        "class CleanAndEnrich(beam.PTransform):\n",
        "    def __init__(self, amount_threshold=0.0): self.amount_threshold = amount_threshold\n",
        "    def expand(self, pcoll):\n",
        "        return (\n",
        "            pcoll\n",
        "            | \"ParseCSV\" >> beam.Map(parse_csv)\n",
        "            | \"DropHeader\" >> beam.Filter(lambda x: x is not None)\n",
        "            | \"FilterAmt\" >> beam.Filter(lambda x: x[\"amount\"] >= self.amount_threshold)\n",
        "            | \"AddRisk\" >> beam.ParDo(AddRiskScore())\n",
        "        )\n"
      ],
      "metadata": {
        "id": "g7SzB33Ocg5F"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# === Cell 5: Clean outputs + build/run pipeline (batch) ===\n",
        "import os, glob, shutil\n",
        "import apache_beam as beam\n",
        "from apache_beam.io import ReadFromText, WriteToText\n",
        "from apache_beam.options.pipeline_options import PipelineOptions\n",
        "from apache_beam.transforms.window import FixedWindows\n",
        "\n",
        "# Output *prefixes* (must NOT be directories)\n",
        "OUTPUT_1 = \"/content/output/high_value_risk\"\n",
        "OUTPUT_2 = \"/content/output/fraud_partition\"\n",
        "OUTPUT_3 = \"/content/output/window_sums\"\n",
        "\n",
        "# Clean any previous files/dirs that collide with prefixes\n",
        "for prefix in [OUTPUT_1, OUTPUT_2, OUTPUT_3]:\n",
        "    for path in glob.glob(prefix + \"*\"):\n",
        "        if os.path.isdir(path):\n",
        "            shutil.rmtree(path, ignore_errors=True)\n",
        "        else:\n",
        "            try:\n",
        "                os.remove(path)\n",
        "            except FileNotFoundError:\n",
        "                pass\n",
        "\n",
        "# Batch mode: no --streaming flag\n",
        "opts = PipelineOptions([\"--runner=DirectRunner\"])\n",
        "\n",
        "with beam.Pipeline(options=opts) as p:\n",
        "    lines = p | \"ReadCSV\" >> ReadFromText(csv_path)\n",
        "\n",
        "    # Composite: parse -> filter -> risk\n",
        "    high_value = lines | \"CleanAndEnrich>=100\" >> CleanAndEnrich(amount_threshold=100.0)\n",
        "\n",
        "    # Partition: fraud vs not fraud\n",
        "    def part_fn(e, n):\n",
        "        return 0 if e[\"is_fraud\"] == 1 else 1\n",
        "    fraud, not_fraud = high_value | \"PartitionFraud\" >> beam.Partition(part_fn, 2)\n",
        "\n",
        "    # Write outputs (prefixes). num_shards=1 for single file each.\n",
        "    fraud      | \"WriteFraud\"    >> WriteToText(OUTPUT_2 + \"_fraud\", num_shards=1)\n",
        "    not_fraud  | \"WriteNotFraud\" >> WriteToText(OUTPUT_2 + \"_notfraud\", num_shards=1)\n",
        "    high_value | \"WriteHighVals\" >> WriteToText(OUTPUT_1, num_shards=1)\n",
        "\n",
        "    # Windowing: 5-minute fixed windows; sum amount by country\n",
        "    _ = (\n",
        "        high_value\n",
        "        | \"ToTimestamped\" >> beam.Map(to_timestamped)\n",
        "        | \"Fixed5mWindows\" >> beam.WindowInto(FixedWindows(5 * 60))\n",
        "        | \"KV(country,amount)\" >> beam.Map(lambda e: (e[\"country\"], e[\"amount\"]))\n",
        "        | \"SumPerWindow\" >> beam.CombinePerKey(sum)\n",
        "        | \"WriteWindowSums\" >> WriteToText(OUTPUT_3, num_shards=1)\n",
        "    )\n",
        "\n",
        "print(\"Done. Files under /content/output\")\n",
        "!ls -lah /content/output | sed -n '1,200p'\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lVqaU3A1clWM",
        "outputId": "4625caa8-e2a5-44ff-bab6-640f35a52e88"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Done. Files under /content/output\n",
            "total 20K\n",
            "drwxr-xr-x 2 root root 4.0K Oct 27 02:27 .\n",
            "drwxr-xr-x 1 root root 4.0K Oct 27 02:14 ..\n",
            "-rw-r--r-- 1 root root  318 Oct 27 02:27 fraud_partition_fraud-00000-of-00001\n",
            "-rw-r--r-- 1 root root    0 Oct 27 02:27 fraud_partition_notfraud-00000-of-00001\n",
            "-rw-r--r-- 1 root root  318 Oct 27 02:27 high_value_risk-00000-of-00001\n",
            "-rw-r--r-- 1 root root   29 Oct 27 02:27 window_sums-00000-of-00001\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# === Cell 6: Preview output files ===\n",
        "\n",
        "print(\"High-value sample:\")\n",
        "!head -n 5 /content/output/high_value_risk-00000-of-00001\n",
        "\n",
        "print(\"\\nFraud partition (fraud):\")\n",
        "!head -n 5 /content/output/fraud_partition_fraud-00000-of-00001\n",
        "\n",
        "print(\"\\nFraud partition (not fraud):\")\n",
        "!head -n 5 /content/output/fraud_partition_notfraud-00000-of-00001\n",
        "\n",
        "print(\"\\nWindow sums sample:\")\n",
        "!head -n 5 /content/output/window_sums-00000-of-00001\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uamWd-d5e_GO",
        "outputId": "9833fa3b-bc75-4beb-bede-50c494f04d22"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "High-value sample:\n",
            "{'ts': '2025-01-01T00:01:25', 'user': 'u2', 'amount': 300.0, 'country': 'US', 'is_fraud': 1, 'risk': 0.5}\n",
            "{'ts': '2025-01-01T00:02:05', 'user': 'u4', 'amount': 700.0, 'country': 'US', 'is_fraud': 1, 'risk': 1.0}\n",
            "{'ts': '2025-01-01T00:05:05', 'user': 'u1', 'amount': 900.0, 'country': 'US', 'is_fraud': 1, 'risk': 1.0}\n",
            "\n",
            "Fraud partition (fraud):\n",
            "{'ts': '2025-01-01T00:01:25', 'user': 'u2', 'amount': 300.0, 'country': 'US', 'is_fraud': 1, 'risk': 0.5}\n",
            "{'ts': '2025-01-01T00:02:05', 'user': 'u4', 'amount': 700.0, 'country': 'US', 'is_fraud': 1, 'risk': 1.0}\n",
            "{'ts': '2025-01-01T00:05:05', 'user': 'u1', 'amount': 900.0, 'country': 'US', 'is_fraud': 1, 'risk': 1.0}\n",
            "\n",
            "Fraud partition (not fraud):\n",
            "\n",
            "Window sums sample:\n",
            "('US', 1000.0)\n",
            "('US', 900.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# cell 7 === RunInference (sklearn) — fixed for Beam >= 2.56 ===\n",
        "import numpy as np, pickle, json\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.pipeline import make_pipeline\n",
        "\n",
        "# tiny model\n",
        "X = np.array([[12.5,1],[300,1],[50,0],[700,1],[25,0],[900,1],[10,0],[75.5,1],[15,1]], dtype=float)\n",
        "y = np.array([0,1,0,1,0,1,0,0,0])\n",
        "clf = make_pipeline(StandardScaler(), LogisticRegression(max_iter=200)).fit(X,y)\n",
        "\n",
        "MODEL_PATH = \"/content/model.pkl\"\n",
        "with open(MODEL_PATH, \"wb\") as f:\n",
        "    pickle.dump(clf, f)\n",
        "\n",
        "from apache_beam.ml.inference.base import RunInference\n",
        "from apache_beam.ml.inference.sklearn_inference import SklearnModelHandlerNumpy\n",
        "from apache_beam.options.pipeline_options import PipelineOptions\n",
        "\n",
        "def to_features(e):\n",
        "    return np.array([e[\"amount\"], 1 if e[\"country\"]==\"US\" else 0], dtype=float)\n",
        "\n",
        "# Use predict_proba so we get class probabilities\n",
        "handler = SklearnModelHandlerNumpy(MODEL_PATH)\n",
        "\n",
        "INF_OUT = \"/content/output/inference_scores\"\n",
        "import os; os.system(\"rm -f \" + INF_OUT + \"*\")\n",
        "\n",
        "def to_json_from_prediction_result(r):\n",
        "    # r is a PredictionResult; probabilities are in r.inference\n",
        "    arr = np.array(r.inference).ravel()\n",
        "    # if two probs (neg, pos), take the positive class prob; else just first value\n",
        "    val = float(arr[-1]) if arr.size > 1 else float(arr[0])\n",
        "    return json.dumps({\"proba_1\": val})\n",
        "\n",
        "with beam.Pipeline(options=PipelineOptions([\"--runner=DirectRunner\"])) as p:\n",
        "    _ = (\n",
        "        p\n",
        "        | \"ReadCSV-Inf\" >> ReadFromText(csv_path)\n",
        "        | \"Clean(>=50)+Risk\" >> CleanAndEnrich(amount_threshold=50.0)\n",
        "        | \"ToFeatures\" >> beam.Map(to_features)\n",
        "        | \"RunInference\" >> RunInference(handler)\n",
        "        | \"ToJSON\" >> beam.Map(to_json_from_prediction_result)\n",
        "        | \"WriteInf\" >> WriteToText(INF_OUT, num_shards=1)\n",
        "    )\n",
        "\n",
        "!echo \"Inference sample:\"\n",
        "!head -n 5 /content/output/inference_scores-00000-of-00001"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vCC9P3zLhFzh",
        "outputId": "7d37d70d-edb1-42c3-8f9b-c0d1c359a1a2"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Inference sample:\n",
            "{\"proba_1\": 0.0}\n",
            "{\"proba_1\": 0.0}\n",
            "{\"proba_1\": 1.0}\n",
            "{\"proba_1\": 1.0}\n",
            "{\"proba_1\": 0.0}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "z6Gb_WDKhH0p"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}