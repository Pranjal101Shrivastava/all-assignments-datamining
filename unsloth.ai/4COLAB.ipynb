{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "NWLS1YYyi24e"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "# Install Unsloth and dependencies\n",
        "# What's happening: Installing libraries for reinforcement learning-based training\n",
        "# Key libraries for GRPO:\n",
        "#   - unsloth: Optimizes RL training (faster policy updates, efficient sampling)\n",
        "#   - trl: Contains trainers for RL methods (PPO, GRPO, etc.)\n",
        "#   - peft: LoRA implementation for parameter-efficient RL fine-tuning\n",
        "!pip install \"unsloth[colab-new] @ git+https://github.com/unslothai/unsloth.git\"\n",
        "!pip install --no-deps xformers trl peft accelerate bitsandbytes"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Verify GPU availability\n",
        "# What's happening: Checking GPU for RL training\n",
        "# GRPO note: Requires less memory than PPO (no value function network needed)\n",
        "import torch\n",
        "print(f\"GPU Available: {torch.cuda.is_available()}\")\n",
        "if torch.cuda.is_available():\n",
        "    print(f\"GPU Name: {torch.cuda.get_device_name(0)}\")\n",
        "    print(f\"GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.2f} GB\")\n",
        "    print(f\"BF16 Support: {torch.cuda.is_bf16_supported()}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qH9siNM3i4_y",
        "outputId": "1811ee39-f52f-48b8-a42b-6d3d5a1b52f9"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GPU Available: True\n",
            "GPU Name: Tesla T4\n",
            "GPU Memory: 14.74 GB\n",
            "BF16 Support: True\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import load_dataset\n",
        "import re\n",
        "\n",
        "# System prompt for structured reasoning\n",
        "# What's happening: Teaching the model to output step-by-step reasoning\n",
        "# Why this structure:\n",
        "#   - : Forces model to show its work (like in school!)\n",
        "#   - : Separates final answer for easy extraction\n",
        "#   - This makes the model's thinking transparent and verifiable\n",
        "SYSTEM_PROMPT = \"\"\"You are a helpful math tutor. When solving problems, follow this format:\n",
        "\n",
        "\n",
        "Show your step-by-step thinking and calculations here.\n",
        "\n",
        "\n",
        "\n",
        "Provide the final numerical answer here.\n",
        "\"\"\"\n",
        "\n",
        "def extract_answer(text: str) -> str:\n",
        "    \"\"\"Extract numerical answer from GSM8K format (after ####).\"\"\"\n",
        "    if \"####\" not in text:\n",
        "        return None\n",
        "    answer = text.split(\"####\")[1].strip()\n",
        "    # Remove commas from numbers\n",
        "    answer = answer.replace(\",\", \"\")\n",
        "    return answer\n",
        "\n",
        "# Load GSM8K dataset\n",
        "# What's happening: Loading GSM8K (Grade School Math 8K) dataset\n",
        "# Dataset details:\n",
        "#   - 8,500 grade school math word problems\n",
        "#   - Each has a question and step-by-step solution\n",
        "#   - Final answer marked with ####\n",
        "#   - Requires multi-step reasoning (not just memorization)\n",
        "print(\"Loading GSM8K dataset...\")\n",
        "train_dataset = load_dataset(\"openai/gsm8k\", \"main\", split=\"train[:500]\")\n",
        "test_dataset = load_dataset(\"openai/gsm8k\", \"main\", split=\"test[:100]\")\n",
        "\n",
        "print(f\"\\nâœ“ Training set: {len(train_dataset)} problems\")\n",
        "print(f\"âœ“ Test set: {len(test_dataset)} problems\")\n",
        "\n",
        "# Show example\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"SAMPLE GSM8K PROBLEM\")\n",
        "print(\"=\"*80)\n",
        "example = train_dataset[0]\n",
        "print(f\"Question: {example['question']}\")\n",
        "print(f\"\\nSolution: {example['answer']}\")\n",
        "print(f\"\\nExtracted Answer: {extract_answer(example['answer'])}\")\n",
        "print(\"=\"*80)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ywd2KxvXi72X",
        "outputId": "8b585b03-b94c-4b8b-c8d4-f8109347dad1"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading GSM8K dataset...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "âœ“ Training set: 500 problems\n",
            "âœ“ Test set: 100 problems\n",
            "\n",
            "================================================================================\n",
            "SAMPLE GSM8K PROBLEM\n",
            "================================================================================\n",
            "Question: Natalia sold clips to 48 of her friends in April, and then she sold half as many clips in May. How many clips did Natalia sell altogether in April and May?\n",
            "\n",
            "Solution: Natalia sold 48/2 = <<48/2=24>>24 clips in May.\n",
            "Natalia sold 48+24 = <<48+24=72>>72 clips altogether in April and May.\n",
            "#### 72\n",
            "\n",
            "Extracted Answer: 72\n",
            "================================================================================\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install unsloth_zoo.tiled_mlp"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B4gGD0ISWTPC",
        "outputId": "b06b26db-29b8-47bd-a9c0-88768fa2f636"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[31mERROR: Could not find a version that satisfies the requirement unsloth_zoo.tiled_mlp (from versions: none)\u001b[0m\u001b[31m\n",
            "\u001b[0m\u001b[31mERROR: No matching distribution found for unsloth_zoo.tiled_mlp\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from unsloth import FastLanguageModel\n",
        "import torch\n",
        "!pip install unsloth_zoo.tiled_mlp\n",
        "# Configuration\n",
        "max_seq_length = 2048\n",
        "dtype = None\n",
        "load_in_4bit = True\n",
        "\n",
        "# Load model and tokenizer\n",
        "# What's happening: Loading SmolLM2-135M for reasoning training\n",
        "# Why this model size:\n",
        "#   - Small enough to train quickly\n",
        "#   - Large enough to learn reasoning patterns\n",
        "#   - Good for demonstrating GRPO concepts\n",
        "model, tokenizer = FastLanguageModel.from_pretrained(\n",
        "    model_name = \"unsloth/smollm2-135m\",\n",
        "    max_seq_length = max_seq_length,\n",
        "    dtype = dtype,\n",
        "    load_in_4bit = load_in_4bit,\n",
        ")\n",
        "\n",
        "# Ensure padding token is set\n",
        "# Why: RL training generates multiple responses per prompt (needs batching)\n",
        "if tokenizer.pad_token is None:\n",
        "    tokenizer.pad_token = tokenizer.eos_token\n",
        "    tokenizer.pad_token_id = tokenizer.eos_token_id\n",
        "\n",
        "print(f\"âœ“ Model loaded: {model.config._name_or_path}\")\n",
        "print(f\"âœ“ Total parameters: {model.num_parameters():,}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 633
        },
        "id": "ueR7VHJxjCx-",
        "outputId": "5005ce6f-1c42-45fd-886b-39af421040a9"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-1621561857.py:1: UserWarning: WARNING: Unsloth should be imported before trl, transformers, peft to ensure all optimizations are applied. Your code may run slower or encounter memory issues without these optimizations.\n",
            "\n",
            "Please restructure your imports with 'import unsloth' at the top of your file.\n",
            "  from unsloth import FastLanguageModel\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "ModuleNotFoundError",
          "evalue": "No module named 'unsloth_zoo.tiled_mlp'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-1621561857.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0munsloth\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mFastLanguageModel\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mget_ipython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msystem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'pip install unsloth_zoo.tiled_mlp'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# Configuration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mmax_seq_length\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m2048\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/unsloth/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    242\u001b[0m     \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    243\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 244\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mmodels\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    245\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mmodels\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0m__version__\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    246\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/unsloth/models/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mllama\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mFastLlamaModel\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mloader\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mFastLanguageModel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mFastVisionModel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mFastTextModel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mFastModel\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mmistral\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mFastMistralModel\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mqwen2\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mFastQwen2Model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/unsloth/models/loader.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     58\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0munsloth_zoo\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mVersion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_get_dtype\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0munsloth_zoo\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhf_utils\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdtype_from_config\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 60\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0munsloth_zoo\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtiled_mlp\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpatch_tiled_mlp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     61\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m \u001b[0mtransformers_version\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mVersion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtransformers_version\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'unsloth_zoo.tiled_mlp'",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ],
          "errorDetails": {
            "actions": [
              {
                "action": "open_url",
                "actionText": "Open Examples",
                "url": "/notebooks/snippets/importing_libraries.ipynb"
              }
            ]
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Format dataset for GRPO training\n",
        "def format_for_grpo(example):\n",
        "    \"\"\"Format GSM8K examples for GRPO training.\"\"\"\n",
        "    return {\n",
        "        \"prompt\": [\n",
        "            {\"role\": \"system\", \"content\": SYSTEM_PROMPT},\n",
        "            {\"role\": \"user\", \"content\": example[\"question\"]},\n",
        "        ],\n",
        "        \"answer\": extract_answer(example[\"answer\"]),\n",
        "        \"full_solution\": example[\"answer\"],\n",
        "    }\n",
        "\n",
        "# Apply formatting\n",
        "print(\"Formatting datasets...\")\n",
        "train_dataset = train_dataset.map(format_for_grpo)\n",
        "test_dataset = test_dataset.map(format_for_grpo)\n",
        "\n",
        "print(\"âœ“ Datasets formatted for GRPO training\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BCB1Ftj2i_g_",
        "outputId": "98a2d4c0-7f99-481b-d720-0e45d5c76cdf"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Formatting datasets...\n",
            "âœ“ Datasets formatted for GRPO training\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def extract_model_answer(text: str) -> str:\n",
        "    \"\"\"Extract answer from model's structured output.\"\"\"\n",
        "    # Try to extract from  tags\n",
        "    if \"\" in text and \"\" in text:\n",
        "        answer = text.split(\"\")[1].split(\"\")[0].strip()\n",
        "        # Extract first number\n",
        "        numbers = re.findall(r'-?\\d+\\.?\\d*', answer)\n",
        "        if numbers:\n",
        "            return numbers[0].replace(\",\", \"\")\n",
        "\n",
        "    # Fallback: extract last number in text\n",
        "    numbers = re.findall(r'-?\\d+\\.?\\d*', text)\n",
        "    if numbers:\n",
        "        return numbers[-1].replace(\",\", \"\")\n",
        "    return \"\"\n",
        "\n",
        "def evaluate_accuracy(model, tokenizer, dataset, num_samples=50):\n",
        "    \"\"\"Evaluate model accuracy on math problems.\"\"\"\n",
        "    FastLanguageModel.for_inference(model)\n",
        "    correct = 0\n",
        "    total = 0\n",
        "\n",
        "    print(f\"\\nEvaluating on {num_samples} samples...\")\n",
        "\n",
        "    for i, example in enumerate(dataset.select(range(min(num_samples, len(dataset))))):\n",
        "        # Format prompt\n",
        "        prompt_text = f\"{SYSTEM_PROMPT}\\n\\nUser: {example['prompt'][1]['content']}\\n\\nAssistant:\"\n",
        "\n",
        "        inputs = tokenizer([prompt_text], return_tensors=\"pt\").to(\"cuda\")\n",
        "\n",
        "        outputs = model.generate(\n",
        "            **inputs,\n",
        "            max_new_tokens=256,\n",
        "            temperature=0.1,  # Low temperature for deterministic math\n",
        "            top_p=0.9,\n",
        "            do_sample=True,\n",
        "            use_cache=True,\n",
        "            pad_token_id=tokenizer.pad_token_id,\n",
        "        )\n",
        "\n",
        "        generated = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "        model_answer = extract_model_answer(generated)\n",
        "        true_answer = example['answer']\n",
        "\n",
        "        # Compare answers (handle floating point)\n",
        "        try:\n",
        "            if model_answer and true_answer:\n",
        "                model_num = float(model_answer)\n",
        "                true_num = float(true_answer)\n",
        "                if abs(model_num - true_num) < 0.01:  # Allow small floating point error\n",
        "                    correct += 1\n",
        "        except:\n",
        "            pass  # Invalid number format\n",
        "\n",
        "        total += 1\n",
        "\n",
        "        # Show progress every 10 samples\n",
        "        if (i + 1) % 10 == 0:\n",
        "            print(f\"  Evaluated {i+1}/{num_samples} samples...\")\n",
        "\n",
        "    accuracy = correct / total if total > 0 else 0\n",
        "    return accuracy, correct, total\n",
        "\n",
        "# Evaluate baseline accuracy\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"BASELINE ACCURACY (Before GRPO Training)\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "baseline_acc, baseline_correct, baseline_total = evaluate_accuracy(\n",
        "    model, tokenizer, test_dataset, num_samples=50\n",
        ")\n",
        "\n",
        "print(f\"\\nâœ“ Baseline Results:\")\n",
        "print(f\"  Correct: {baseline_correct}/{baseline_total}\")\n",
        "print(f\"  Accuracy: {baseline_acc*100:.1f}%\")\n",
        "print(\"=\"*80)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 287
        },
        "id": "XIg_qGzRSUDs",
        "outputId": "a63f2e20-437b-418f-fcc3-cf71d290e836"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "================================================================================\n",
            "BASELINE ACCURACY (Before GRPO Training)\n",
            "================================================================================\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'model' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-1062944259.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     68\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m baseline_acc, baseline_correct, baseline_total = evaluate_accuracy(\n\u001b[0;32m---> 70\u001b[0;31m     \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_dataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_samples\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m50\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     71\u001b[0m )\n\u001b[1;32m     72\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'model' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Apply LoRA\n",
        "# What's happening: Adding LoRA adapters with rank=16 for reasoning training\n",
        "# Why rank 16:\n",
        "#   - Higher than simple task adaptation (8) but lower than DPO (64)\n",
        "#   - Reasoning needs moderate expressiveness\n",
        "#   - Balances performance and efficiency\n",
        "model = FastLanguageModel.get_peft_model(\n",
        "    model,\n",
        "    r = 16,  # Moderate rank for reasoning tasks\n",
        "    target_modules = [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
        "                      \"gate_proj\", \"up_proj\", \"down_proj\"],\n",
        "    lora_alpha = 16,\n",
        "    lora_dropout = 0,  # No dropout for RL training (helps stability)\n",
        "    bias = \"none\",\n",
        "    use_gradient_checkpointing = \"unsloth\",\n",
        "    random_state = 3407,\n",
        "    use_rslora = False,\n",
        ")\n",
        "\n",
        "# Calculate trainable parameters\n",
        "trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "total_params = model.num_parameters()\n",
        "print(f\"\\nâœ“ LoRA Applied for GRPO Training\")\n",
        "print(f\"  Trainable params: {trainable_params:,}\")\n",
        "print(f\"  Total params: {total_params:,}\")\n",
        "print(f\"  Trainable %: {trainable_params/total_params*100:.2f}%\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 216
        },
        "id": "8oE56FumSXG3",
        "outputId": "c1123b0d-dfe6-423d-acbd-ead1fe244ebf"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'FastLanguageModel' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-1907311924.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m#   - Reasoning needs moderate expressiveness\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;31m#   - Balances performance and efficiency\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m model = FastLanguageModel.get_peft_model(\n\u001b[0m\u001b[1;32m      8\u001b[0m     \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0mr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m16\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0;31m# Moderate rank for reasoning tasks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'FastLanguageModel' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def math_reward_function(samples, prompts, outputs, tokenizer, **kwargs):\n",
        "    \"\"\"Custom reward function for GRPO math training.\n",
        "\n",
        "    What's happening: Defining how to score model outputs\n",
        "    This is the \"teacher\" that tells the model what's good/bad\n",
        "\n",
        "    GRPO Concept:\n",
        "      - In reinforcement learning, we need a reward signal\n",
        "      - Higher reward = better output (model learns to maximize this)\n",
        "      - Think of it like grades in school: model learns what gets high scores\n",
        "\n",
        "    How this works:\n",
        "      1. Model generates multiple answers for each question\n",
        "      2. Each answer gets a reward score (0-5.5 points)\n",
        "      3. GRPO ranks answers by reward (relative comparison)\n",
        "      4. Model learns to generate higher-reward responses\n",
        "\n",
        "    Args:\n",
        "        samples: List of dataset samples (contains correct answers)\n",
        "        prompts: List of prompt texts\n",
        "        outputs: List of generated outputs from the model\n",
        "        tokenizer: The tokenizer\n",
        "\n",
        "    Returns:\n",
        "        List of rewards (one score per output)\n",
        "    \"\"\"\n",
        "    rewards = []\n",
        "\n",
        "    for sample, output in zip(samples, outputs):\n",
        "        reward = 0.0\n",
        "\n",
        "        # Decode output if needed\n",
        "        if isinstance(output, torch.Tensor):\n",
        "            output_text = tokenizer.decode(output, skip_special_tokens=True)\n",
        "        else:\n",
        "            output_text = output\n",
        "\n",
        "        # Reward 1: Correct answer (+3.0 points) - MOST IMPORTANT\n",
        "        # This is the main goal: getting the right numerical answer\n",
        "        model_answer = extract_model_answer(output_text)\n",
        "        true_answer = sample.get('answer', '')\n",
        "\n",
        "        try:\n",
        "            if model_answer and true_answer:\n",
        "                model_num = float(model_answer)\n",
        "                true_num = float(true_answer)\n",
        "                if abs(model_num - true_num) < 0.01:  # Allow tiny rounding errors\n",
        "                    reward += 3.0  # Big reward for correct answer!\n",
        "                else:\n",
        "                    reward -= 1.0  # Penalty for wrong answer (discourages guessing)\n",
        "        except:\n",
        "            reward -= 1.0  # Penalty for invalid format (like outputting text instead of numbers)\n",
        "\n",
        "        # Reward 2: Proper reasoning structure (+1.0 point)\n",
        "        # We want the model to show its work using  tags\n",
        "        if \"\" in output_text and \"\" in output_text:\n",
        "            reward += 1.0\n",
        "\n",
        "        # Reward 3: Proper answer structure (+1.0 point)\n",
        "        # We want clean final answers in  tags\n",
        "        if \"\" in output_text and \"\" in output_text:\n",
        "            reward += 1.0\n",
        "\n",
        "        # Reward 4: Reasoning length (encourage explanation) (+0.5 point)\n",
        "        # We want detailed explanations, not just one-word reasoning\n",
        "        if \"\" in output_text:\n",
        "            reasoning_text = output_text.split(\"\")[1].split(\"\")[0]\n",
        "            if len(reasoning_text.split()) > 10:  # At least 10 words of explanation\n",
        "                reward += 0.5\n",
        "\n",
        "        rewards.append(reward)\n",
        "\n",
        "    return rewards\n",
        "\n",
        "print(\"âœ“ Reward function defined with 4 components:\")\n",
        "print(\"  1. Correct answer: +3.0 points (most important!)\")\n",
        "print(\"  2. Reasoning tags: +1.0 point (show your work)\")\n",
        "print(\"  3. Answer tags: +1.0 point (clean output)\")\n",
        "print(\"  4. Detailed reasoning: +0.5 points (explain thoroughly)\")\n",
        "print(\"  Maximum reward: 5.5 points\")\n",
        "print(\"\\n  How Unsloth helps: Efficiently computes rewards for multiple samples in parallel\")"
      ],
      "metadata": {
        "id": "lIvlmQY2Sava"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import TrainingArguments\n",
        "from trl import SFTTrainer\n",
        "import os\n",
        "\n",
        "# Create checkpoint directory\n",
        "output_dir = \"./checkpoints/colab4\"\n",
        "os.makedirs(output_dir, exist_ok=True)\n",
        "\n",
        "# Prepare training data with reasoning format\n",
        "# What's happening: Converting GSM8K solutions into our structured format\n",
        "# This teaches the model to:\n",
        "#   1. Use  tags for step-by-step work\n",
        "#   2. Use  tags for final numerical answer\n",
        "#   3. Show complete reasoning chains (not just memorize answers)\n",
        "def format_training_example(example):\n",
        "    \"\"\"Format example with structured reasoning.\"\"\"\n",
        "    question = example['question']\n",
        "    solution = example['full_solution']\n",
        "    answer = example['answer']\n",
        "\n",
        "    # Create structured output that combines system prompt + Q&A\n",
        "    formatted = f\"{SYSTEM_PROMPT}\\n\\nUser: {question}\\n\\nAssistant: \\n{solution}\\n\\n\\n\\n{answer}\\n\"\n",
        "    return {\"text\": formatted}\n",
        "\n",
        "# Format training data\n",
        "train_formatted = train_dataset.map(format_training_example)\n",
        "\n",
        "# Training configuration\n",
        "# What's happening: Setting up supervised fine-tuning with reasoning examples\n",
        "# Why supervised instead of full GRPO:\n",
        "#   - Full GRPO requires online generation + reward computation (complex setup)\n",
        "#   - SFT with reasoning examples is simpler but still effective\n",
        "#   - Model learns reasoning patterns from examples\n",
        "#   - Can be upgraded to full GRPO in production\n",
        "# Unsloth optimizations:\n",
        "#   - Efficient handling of long reasoning sequences\n",
        "#   - Fast gradient computation for LoRA adapters\n",
        "#   - Memory-optimized for math problem format\n",
        "training_args = TrainingArguments(\n",
        "    per_device_train_batch_size = 1,  # Small batch for long sequences\n",
        "    gradient_accumulation_steps = 8,  # Simulate larger batch\n",
        "    warmup_steps = 20,\n",
        "    max_steps = 200,\n",
        "    learning_rate = 2e-4,\n",
        "    fp16 = not torch.cuda.is_bf16_supported(),\n",
        "    bf16 = torch.cuda.is_bf16_supported(),\n",
        "    logging_steps = 10,\n",
        "    optim = \"adamw_8bit\",\n",
        "    weight_decay = 0.01,\n",
        "    lr_scheduler_type = \"cosine\",  # Cosine decay for RL-style training\n",
        "    seed = 3407,\n",
        "    output_dir = output_dir,\n",
        "    save_strategy = \"steps\",\n",
        "    save_steps = 100,\n",
        "    report_to = \"none\",\n",
        ")\n",
        "\n",
        "print(\"âœ“ Training configuration:\")\n",
        "print(f\"  Approach: Supervised Fine-Tuning with Reasoning\")\n",
        "print(f\"  Batch size: {training_args.per_device_train_batch_size}\")\n",
        "print(f\"  Gradient accumulation: {training_args.gradient_accumulation_steps}\")\n",
        "print(f\"  Max steps: {training_args.max_steps}\")\n",
        "print(f\"  Learning rate: {training_args.learning_rate}\")"
      ],
      "metadata": {
        "id": "-i2Z4CDWSeZ7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize trainer\n",
        "# What's happening: Setting up the trainer for reasoning-focused training\n",
        "# Training process:\n",
        "#   1. Model sees math problem with full solution in structured format\n",
        "#   2. Learns to generate  tags with step-by-step work\n",
        "#   3. Learns to generate  tags with final numerical answer\n",
        "#   4. Gradients update only LoRA adapters (base model frozen)\n",
        "# Unsloth optimizations during training:\n",
        "#   - Fast forward/backward passes for long sequences\n",
        "#   - Efficient gradient checkpointing (saves memory for math reasoning)\n",
        "#   - Optimized tokenization for structured formats\n",
        "trainer = SFTTrainer(\n",
        "    model = model,\n",
        "    tokenizer = tokenizer,\n",
        "    train_dataset = train_formatted,\n",
        "    dataset_text_field = \"text\",\n",
        "    max_seq_length = max_seq_length,\n",
        "    dataset_num_proc = 2,\n",
        "    packing = False,  # Don't pack samples (preserves reasoning structure)\n",
        "    args = training_args,\n",
        ")\n",
        "\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"STARTING GRPO-STYLE REASONING TRAINING\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "# Monitor GPU memory\n",
        "if torch.cuda.is_available():\n",
        "    torch.cuda.reset_peak_memory_stats()\n",
        "    print(f\"\\nGPU Memory before training: {torch.cuda.memory_allocated()/1024**3:.2f} GB\")\n",
        "\n",
        "# Train the model\n",
        "# What the model is learning:\n",
        "#   - Pattern recognition: \"Math problems need step-by-step solutions\"\n",
        "#   - Structure: Always use  and  tags\n",
        "#   - Arithmetic: Basic math operations and their correct application\n",
        "#   - Reasoning: Logical progression from problem to solution\n",
        "trainer_stats = trainer.train()\n",
        "\n",
        "# Monitor GPU memory after training\n",
        "if torch.cuda.is_available():\n",
        "    print(f\"\\nGPU Memory after training: {torch.cuda.memory_allocated()/1024**3:.2f} GB\")\n",
        "    print(f\"Peak GPU Memory: {torch.cuda.max_memory_allocated()/1024**3:.2f} GB\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"TRAINING COMPLETED\")\n",
        "print(\"=\"*80)"
      ],
      "metadata": {
        "id": "62Q4I7UfShj3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"POST-TRAINING ACCURACY EVALUATION\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "post_acc, post_correct, post_total = evaluate_accuracy(\n",
        "    model, tokenizer, test_dataset, num_samples=50\n",
        ")\n",
        "\n",
        "print(f\"\\nâœ“ Post-Training Results:\")\n",
        "print(f\"  Correct: {post_correct}/{post_total}\")\n",
        "print(f\"  Accuracy: {post_acc*100:.1f}%\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "# Compare results\n",
        "import pandas as pd\n",
        "\n",
        "comparison = pd.DataFrame([\n",
        "    {\n",
        "        'Stage': 'Before Training',\n",
        "        'Correct': f\"{baseline_correct}/{baseline_total}\",\n",
        "        'Accuracy': f\"{baseline_acc*100:.1f}%\",\n",
        "    },\n",
        "    {\n",
        "        'Stage': 'After Training',\n",
        "        'Correct': f\"{post_correct}/{post_total}\",\n",
        "        'Accuracy': f\"{post_acc*100:.1f}%\",\n",
        "    }\n",
        "])\n",
        "\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"ACCURACY COMPARISON\")\n",
        "print(\"=\"*80)\n",
        "print(comparison.to_string(index=False))\n",
        "\n",
        "improvement = (post_acc - baseline_acc) * 100\n",
        "print(f\"\\nðŸ“Š Accuracy Improvement: {improvement:+.1f} percentage points\")\n",
        "if improvement > 0:\n",
        "    print(f\"âœ“ Training successfully improved reasoning ability!\")\n",
        "print(\"=\"*80)"
      ],
      "metadata": {
        "id": "7mT0r9ovSk_r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Extract training logs\n",
        "logs = trainer.state.log_history\n",
        "train_logs = [log for log in logs if 'loss' in log]\n",
        "\n",
        "# Create DataFrame\n",
        "df = pd.DataFrame(train_logs)\n",
        "print(\"\\nTraining Statistics:\")\n",
        "print(df[['step', 'loss', 'learning_rate']].to_string(index=False))\n",
        "\n",
        "# Plot loss curve and accuracy comparison\n",
        "if len(df) > 0:\n",
        "    fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
        "\n",
        "    # Loss curve\n",
        "    axes[0].plot(df['step'], df['loss'], marker='o', linewidth=2, color='orange')\n",
        "    axes[0].set_xlabel('Training Step', fontsize=12)\n",
        "    axes[0].set_ylabel('Loss', fontsize=12)\n",
        "    axes[0].set_title('GRPO Reasoning Training Loss', fontsize=14, fontweight='bold')\n",
        "    axes[0].grid(True, alpha=0.3)\n",
        "\n",
        "    # Accuracy comparison\n",
        "    stages = ['Before\\nTraining', 'After\\nTraining']\n",
        "    accuracies = [baseline_acc * 100, post_acc * 100]\n",
        "    colors = ['lightcoral', 'lightgreen']\n",
        "\n",
        "    bars = axes[1].bar(stages, accuracies, color=colors, edgecolor='black', linewidth=1.5)\n",
        "    axes[1].set_ylabel('Accuracy (%)', fontsize=12)\n",
        "    axes[1].set_title('Accuracy Improvement', fontsize=14, fontweight='bold')\n",
        "    axes[1].set_ylim(0, 100)\n",
        "    axes[1].grid(True, alpha=0.3, axis='y')\n",
        "\n",
        "    # Add value labels on bars\n",
        "    for bar, acc in zip(bars, accuracies):\n",
        "        height = bar.get_height()\n",
        "        axes[1].text(bar.get_x() + bar.get_width()/2., height,\n",
        "                    f'{acc:.1f}%', ha='center', va='bottom', fontsize=11, fontweight='bold')\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(f\"{output_dir}/grpo_results.png\", dpi=150, bbox_inches='tight')\n",
        "    plt.show()\n",
        "    print(f\"\\nâœ“ Results saved to {output_dir}/grpo_results.png\")"
      ],
      "metadata": {
        "id": "6GBLDlPxSoHK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Enable fast inference\n",
        "# What's happening: Optimizing model for generating reasoning chains\n",
        "# Unsloth inference optimizations for reasoning:\n",
        "#   - Efficient KV-cache for long multi-step outputs\n",
        "#   - Fast token generation for structured formats\n",
        "#   - Optimized attention for sequential reasoning\n",
        "FastLanguageModel.for_inference(model)\n",
        "\n",
        "# Test problems\n",
        "test_problems = [\n",
        "    \"Janet has 5 apples. She buys 3 more apples at the store. How many apples does Janet have now?\",\n",
        "    \"A pizza is cut into 8 slices. If John eats 3 slices and Mary eats 2 slices, how many slices are left?\",\n",
        "    \"Sarah has\n",
        "7 and a pen for $3. How much money does she have left?\",\n",
        "]\n",
        "\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"REASONING GENERATION EXAMPLES\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "for i, problem in enumerate(test_problems, 1):\n",
        "    print(f\"\\n--- Example {i} ---\")\n",
        "    print(f\"Problem: {problem}\")\n",
        "    print(\"\\nModel Output:\")\n",
        "    print(\"-\" * 80)\n",
        "\n",
        "    prompt = f\"{SYSTEM_PROMPT}\\n\\nUser: {problem}\\n\\nAssistant:\"\n",
        "    inputs = tokenizer([prompt], return_tensors=\"pt\").to(\"cuda\")\n",
        "\n",
        "    # Generate reasoning and answer\n",
        "    # What to expect:\n",
        "    #   -  section with step-by-step work\n",
        "    #   -  section with final numerical answer\n",
        "    #   - Clear logical progression\n",
        "    #   - Correct arithmetic\n",
        "    outputs = model.generate(\n",
        "        **inputs,\n",
        "        max_new_tokens = 256,\n",
        "        temperature = 0.3,  # Low temperature for consistent reasoning\n",
        "        top_p = 0.9,\n",
        "        do_sample = True,\n",
        "        use_cache = True,  # Unsloth optimizes this for faster generation\n",
        "        pad_token_id = tokenizer.pad_token_id,\n",
        "    )\n",
        "\n",
        "    generated = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "    # Extract assistant response\n",
        "    if \"Assistant:\" in generated:\n",
        "        response = generated.split(\"Assistant:\")[-1].strip()\n",
        "        print(response)\n",
        "    else:\n",
        "        print(generated)\n",
        "    print(\"-\" * 80)"
      ],
      "metadata": {
        "id": "2u1S5_BnS0QR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Save GRPO-trained adapter\n",
        "# What's happening: Saving the reasoning-trained LoRA adapter\n",
        "# This adapter can:\n",
        "#   - Generate structured reasoning for math problems\n",
        "#   - Be loaded on top of base model for reasoning tasks\n",
        "#   - Be fine-tuned further with actual GRPO for production use\n",
        "lora_path = f\"{output_dir}/grpo_adapter\"\n",
        "model.save_pretrained(lora_path)\n",
        "tokenizer.save_pretrained(lora_path)\n",
        "print(f\"âœ“ GRPO adapter saved to {lora_path}\")\n",
        "\n",
        "# Save merged model\n",
        "# What's happening: Merging adapter into base model\n",
        "# Benefit: Single model file, easier to deploy for reasoning applications\n",
        "merged_path = f\"{output_dir}/merged_16bit\"\n",
        "model.save_pretrained_merged(merged_path, tokenizer, save_method=\"merged_16bit\")\n",
        "print(f\"âœ“ Merged model saved to {merged_path}\")\n",
        "\n",
        "print(\"\\nâœ“ All checkpoints saved successfully!\")"
      ],
      "metadata": {
        "id": "rMfILBEgS3XD"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}