{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":1052144,"sourceType":"datasetVersion","datasetId":582088}],"dockerImageVersionId":31153,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip -q install -U autogluon.tabular==1.4.0\nimport pandas as pd\nfrom autogluon.tabular import TabularPredictor\n\ndf = pd.read_csv(\"/kaggle/input/advertisingcsv/Advertising.csv\")\ndf = df.rename(columns={\"Sales\":\"label\"})\npred = TabularPredictor(label=\"label\", problem_type=\"regression\", path=\"ag_ads\").fit(\n    df, time_limit=120, presets=\"medium_quality\", verbosity=2\n)\npred.leaderboard(silent=True).head(10)\n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-11-02T23:20:39.987399Z","iopub.execute_input":"2025-11-02T23:20:39.987706Z","iopub.status.idle":"2025-11-02T23:21:30.509142Z","shell.execute_reply.started":"2025-11-02T23:20:39.987681Z","shell.execute_reply":"2025-11-02T23:21:30.507784Z"}},"outputs":[{"name":"stdout","text":"\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m487.3/487.3 kB\u001b[0m \u001b[31m13.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m225.1/225.1 kB\u001b[0m \u001b[31m10.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m64.2/64.2 kB\u001b[0m \u001b[31m3.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m71.0/71.0 kB\u001b[0m \u001b[31m3.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m9.7/9.7 MB\u001b[0m \u001b[31m78.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m278.0/278.0 kB\u001b[0m \u001b[31m10.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\ncategory-encoders 2.7.0 requires scikit-learn<1.6.0,>=1.0.0, but you have scikit-learn 1.7.2 which is incompatible.\ncesium 0.12.4 requires numpy<3.0,>=2.0, but you have numpy 1.26.4 which is incompatible.\nsklearn-compat 0.1.3 requires scikit-learn<1.7,>=1.2, but you have scikit-learn 1.7.2 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0m","output_type":"stream"},{"name":"stderr","text":"Verbosity: 2 (Standard Logging)\n=================== System Info ===================\nAutoGluon Version:  1.4.0\nPython Version:     3.11.13\nOperating System:   Linux\nPlatform Machine:   x86_64\nPlatform Version:   #1 SMP PREEMPT_DYNAMIC Sun Nov 10 10:07:59 UTC 2024\nCPU Count:          4\nMemory Avail:       30.01 GB / 31.35 GB (95.7%)\nDisk Space Avail:   19.50 GB / 19.52 GB (99.9%)\n===================================================\nPresets specified: ['medium_quality']\nUsing hyperparameters preset: hyperparameters='default'\nBeginning AutoGluon training ... Time limit = 120s\nAutoGluon will save models to \"/kaggle/working/ag_ads\"\nTrain Data Rows:    200\nTrain Data Columns: 4\nLabel Column:       label\nProblem Type:       regression\nPreprocessing data ...\nUsing Feature Generators to preprocess the data ...\nFitting AutoMLPipelineFeatureGenerator...\n\tAvailable Memory:                    30725.47 MB\n\tTrain Data (Original)  Memory Usage: 0.01 MB (0.0% of available memory)\n\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n\tStage 1 Generators:\n\t\tFitting AsTypeFeatureGenerator...\n\tStage 2 Generators:\n\t\tFitting FillNaFeatureGenerator...\n\tStage 3 Generators:\n\t\tFitting IdentityFeatureGenerator...\n\tStage 4 Generators:\n\t\tFitting DropUniqueFeatureGenerator...\n\tStage 5 Generators:\n\t\tFitting DropDuplicatesFeatureGenerator...\n\tTypes of features in original data (raw dtype, special dtypes):\n\t\t('float', []) : 3 | ['TV', 'Radio', 'Newspaper']\n\t\t('int', [])   : 1 | ['Unnamed: 0']\n\tTypes of features in processed data (raw dtype, special dtypes):\n\t\t('float', []) : 3 | ['TV', 'Radio', 'Newspaper']\n\t\t('int', [])   : 1 | ['Unnamed: 0']\n\t0.0s = Fit runtime\n\t4 features in original data used to generate 4 features in processed data.\n\tTrain Data (Processed) Memory Usage: 0.01 MB (0.0% of available memory)\nData preprocessing and feature engineering runtime = 0.07s ...\nAutoGluon will gauge predictive performance using evaluation metric: 'root_mean_squared_error'\n\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n\tTo change this, specify the eval_metric parameter of Predictor()\nAutomatically generating train/validation split with holdout_frac=0.2, Train Rows: 160, Val Rows: 40\nUser-specified model hyperparameters to be fit:\n{\n\t'NN_TORCH': [{}],\n\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, {'learning_rate': 0.03, 'num_leaves': 128, 'feature_fraction': 0.9, 'min_data_in_leaf': 3, 'ag_args': {'name_suffix': 'Large', 'priority': 0, 'hyperparameter_tune_kwargs': None}}],\n\t'CAT': [{}],\n\t'XGB': [{}],\n\t'FASTAI': [{}],\n\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n}\nFitting 9 L1 models, fit_strategy=\"sequential\" ...\nFitting model: LightGBMXT ... Training model for up to 119.93s of the 119.93s of remaining time.\n\tFitting with cpus=2, gpus=0, mem=0.0/29.8 GB\n\t-1.2831\t = Validation score   (-root_mean_squared_error)\n\t10.79s\t = Training   runtime\n\t0.0s\t = Validation runtime\nFitting model: LightGBM ... Training model for up to 109.12s of the 109.12s of remaining time.\n\tFitting with cpus=2, gpus=0, mem=0.0/29.7 GB\n","output_type":"stream"},{"name":"stdout","text":"[1000]\tvalid_set's rmse: 1.29163\n","output_type":"stream"},{"name":"stderr","text":"\t-1.2674\t = Validation score   (-root_mean_squared_error)\n\t0.34s\t = Training   runtime\n\t0.0s\t = Validation runtime\nFitting model: RandomForestMSE ... Training model for up to 108.77s of the 108.76s of remaining time.\n\tFitting with cpus=4, gpus=0\n\t-0.9901\t = Validation score   (-root_mean_squared_error)\n\t0.78s\t = Training   runtime\n\t0.08s\t = Validation runtime\nFitting model: CatBoost ... Training model for up to 107.89s of the 107.89s of remaining time.\n\tFitting with cpus=2, gpus=0\n\t-1.0986\t = Validation score   (-root_mean_squared_error)\n\t1.57s\t = Training   runtime\n\t0.0s\t = Validation runtime\nFitting model: ExtraTreesMSE ... Training model for up to 106.31s of the 106.31s of remaining time.\n\tFitting with cpus=4, gpus=0\n\t-0.8621\t = Validation score   (-root_mean_squared_error)\n\t0.72s\t = Training   runtime\n\t0.08s\t = Validation runtime\nFitting model: NeuralNetFastAI ... Training model for up to 105.49s of the 105.49s of remaining time.\n\tFitting with cpus=2, gpus=0, mem=0.0/29.7 GB\n\t-1.1297\t = Validation score   (-root_mean_squared_error)\n\t3.53s\t = Training   runtime\n\t0.02s\t = Validation runtime\nFitting model: XGBoost ... Training model for up to 101.93s of the 101.92s of remaining time.\n\tFitting with cpus=2, gpus=0\n\t-1.0751\t = Validation score   (-root_mean_squared_error)\n\t0.29s\t = Training   runtime\n\t0.0s\t = Validation runtime\nFitting model: NeuralNetTorch ... Training model for up to 101.62s of the 101.62s of remaining time.\n\tFitting with cpus=2, gpus=0, mem=0.0/29.7 GB\n/usr/local/lib/python3.11/dist-packages/sklearn/compose/_column_transformer.py:975: FutureWarning: The parameter `force_int_remainder_cols` is deprecated and will be removed in 1.9. It has no effect. Leave it to its default value to avoid this warning.\n  warnings.warn(\n\t-1.0001\t = Validation score   (-root_mean_squared_error)\n\t8.66s\t = Training   runtime\n\t0.0s\t = Validation runtime\nFitting model: LightGBMLarge ... Training model for up to 92.94s of the 92.94s of remaining time.\n\tFitting with cpus=2, gpus=0, mem=0.0/29.6 GB\n","output_type":"stream"},{"name":"stdout","text":"[1000]\tvalid_set's rmse: 1.01726\n","output_type":"stream"},{"name":"stderr","text":"\t-1.0168\t = Validation score   (-root_mean_squared_error)\n\t1.87s\t = Training   runtime\n\t0.01s\t = Validation runtime\nFitting model: WeightedEnsemble_L2 ... Training model for up to 119.93s of the 90.90s of remaining time.\n\tEnsemble Weights: {'ExtraTreesMSE': 0.833, 'LightGBMLarge': 0.125, 'NeuralNetTorch': 0.042}\n\t-0.8584\t = Validation score   (-root_mean_squared_error)\n\t0.01s\t = Training   runtime\n\t0.0s\t = Validation runtime\nAutoGluon training complete, total runtime = 29.15s ... Best model: WeightedEnsemble_L2 | Estimated inference throughput: 430.2 rows/s (40 batch size)\nTabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"/kaggle/working/ag_ads\")\n","output_type":"stream"},{"execution_count":1,"output_type":"execute_result","data":{"text/plain":"                 model  score_val              eval_metric  pred_time_val  \\\n0  WeightedEnsemble_L2  -0.858382  root_mean_squared_error       0.092977   \n1        ExtraTreesMSE  -0.862064  root_mean_squared_error       0.081115   \n2      RandomForestMSE  -0.990141  root_mean_squared_error       0.078790   \n3       NeuralNetTorch  -1.000052  root_mean_squared_error       0.004802   \n4        LightGBMLarge  -1.016835  root_mean_squared_error       0.006562   \n5              XGBoost  -1.075068  root_mean_squared_error       0.003785   \n6             CatBoost  -1.098595  root_mean_squared_error       0.003648   \n7      NeuralNetFastAI  -1.129652  root_mean_squared_error       0.019548   \n8             LightGBM  -1.267441  root_mean_squared_error       0.001579   \n9           LightGBMXT  -1.283088  root_mean_squared_error       0.002354   \n\n    fit_time  pred_time_val_marginal  fit_time_marginal  stack_level  \\\n0  11.267929                0.000499           0.013464            2   \n1   0.720229                0.081115           0.720229            1   \n2   0.776917                0.078790           0.776917            1   \n3   8.664112                0.004802           8.664112            1   \n4   1.870124                0.006562           1.870124            1   \n5   0.287748                0.003785           0.287748            1   \n6   1.566943                0.003648           1.566943            1   \n7   3.526216                0.019548           3.526216            1   \n8   0.340653                0.001579           0.340653            1   \n9  10.792495                0.002354          10.792495            1   \n\n   can_infer  fit_order  \n0       True         10  \n1       True          5  \n2       True          3  \n3       True          8  \n4       True          9  \n5       True          7  \n6       True          4  \n7       True          6  \n8       True          2  \n9       True          1  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>model</th>\n      <th>score_val</th>\n      <th>eval_metric</th>\n      <th>pred_time_val</th>\n      <th>fit_time</th>\n      <th>pred_time_val_marginal</th>\n      <th>fit_time_marginal</th>\n      <th>stack_level</th>\n      <th>can_infer</th>\n      <th>fit_order</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>WeightedEnsemble_L2</td>\n      <td>-0.858382</td>\n      <td>root_mean_squared_error</td>\n      <td>0.092977</td>\n      <td>11.267929</td>\n      <td>0.000499</td>\n      <td>0.013464</td>\n      <td>2</td>\n      <td>True</td>\n      <td>10</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>ExtraTreesMSE</td>\n      <td>-0.862064</td>\n      <td>root_mean_squared_error</td>\n      <td>0.081115</td>\n      <td>0.720229</td>\n      <td>0.081115</td>\n      <td>0.720229</td>\n      <td>1</td>\n      <td>True</td>\n      <td>5</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>RandomForestMSE</td>\n      <td>-0.990141</td>\n      <td>root_mean_squared_error</td>\n      <td>0.078790</td>\n      <td>0.776917</td>\n      <td>0.078790</td>\n      <td>0.776917</td>\n      <td>1</td>\n      <td>True</td>\n      <td>3</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>NeuralNetTorch</td>\n      <td>-1.000052</td>\n      <td>root_mean_squared_error</td>\n      <td>0.004802</td>\n      <td>8.664112</td>\n      <td>0.004802</td>\n      <td>8.664112</td>\n      <td>1</td>\n      <td>True</td>\n      <td>8</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>LightGBMLarge</td>\n      <td>-1.016835</td>\n      <td>root_mean_squared_error</td>\n      <td>0.006562</td>\n      <td>1.870124</td>\n      <td>0.006562</td>\n      <td>1.870124</td>\n      <td>1</td>\n      <td>True</td>\n      <td>9</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>XGBoost</td>\n      <td>-1.075068</td>\n      <td>root_mean_squared_error</td>\n      <td>0.003785</td>\n      <td>0.287748</td>\n      <td>0.003785</td>\n      <td>0.287748</td>\n      <td>1</td>\n      <td>True</td>\n      <td>7</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>CatBoost</td>\n      <td>-1.098595</td>\n      <td>root_mean_squared_error</td>\n      <td>0.003648</td>\n      <td>1.566943</td>\n      <td>0.003648</td>\n      <td>1.566943</td>\n      <td>1</td>\n      <td>True</td>\n      <td>4</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>NeuralNetFastAI</td>\n      <td>-1.129652</td>\n      <td>root_mean_squared_error</td>\n      <td>0.019548</td>\n      <td>3.526216</td>\n      <td>0.019548</td>\n      <td>3.526216</td>\n      <td>1</td>\n      <td>True</td>\n      <td>6</td>\n    </tr>\n    <tr>\n      <th>8</th>\n      <td>LightGBM</td>\n      <td>-1.267441</td>\n      <td>root_mean_squared_error</td>\n      <td>0.001579</td>\n      <td>0.340653</td>\n      <td>0.001579</td>\n      <td>0.340653</td>\n      <td>1</td>\n      <td>True</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>9</th>\n      <td>LightGBMXT</td>\n      <td>-1.283088</td>\n      <td>root_mean_squared_error</td>\n      <td>0.002354</td>\n      <td>10.792495</td>\n      <td>0.002354</td>\n      <td>10.792495</td>\n      <td>1</td>\n      <td>True</td>\n      <td>1</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}],"execution_count":1}]}