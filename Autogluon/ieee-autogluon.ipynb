{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":13475486,"sourceType":"datasetVersion","datasetId":8554816}],"dockerImageVersionId":31153,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# ===== Install =====\n!pip -q install -U pip\n!pip -q install -U autogluon.tabular==1.4.0\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-02T23:32:28.240460Z","iopub.execute_input":"2025-11-02T23:32:28.241342Z","iopub.status.idle":"2025-11-02T23:32:36.761619Z","shell.execute_reply.started":"2025-11-02T23:32:28.241304Z","shell.execute_reply":"2025-11-02T23:32:36.760163Z"}},"outputs":[],"execution_count":1},{"cell_type":"code","source":"# ===== Load & Merge (sampled) =====\nimport pandas as pd, numpy as np\nfrom autogluon.tabular import TabularPredictor\n\nINPUT = \"/kaggle/input/ieee-fraud-detection\"\n\nprint(\"Loading...\")\ntrain_trans = pd.read_csv(f\"{INPUT}/train_transaction.csv\")\ntest_trans  = pd.read_csv(f\"{INPUT}/test_transaction.csv\")\ntrain_id    = pd.read_csv(f\"{INPUT}/train_identity.csv\")\ntest_id     = pd.read_csv(f\"{INPUT}/test_identity.csv\")\n\n# Merge identity info\ntrain = train_trans.merge(train_id, how=\"left\", on=\"TransactionID\")\ntest  = test_trans.merge(test_id,  how=\"left\", on=\"TransactionID\")\n\nprint(\"Shapes before sampling:\", train.shape, test.shape)\n\n# === Reduce size ===\n# 1) Use 10% of training (random sample)\ntrain = train.sample(frac=0.1, random_state=42)\n\n# 2) Drop high-NA columns (>95%)\nna_thresh = 0.95\ndrop_cols = [c for c in train.columns if c != \"isFraud\" and train[c].isna().mean() > na_thresh]\ntrain = train.drop(columns=drop_cols)\ntest  = test.drop(columns=[c for c in drop_cols if c in test.columns])\n\n# 3) Fill remaining NaNs with -999 (simple, fast)\ntrain = train.fillna(-999)\ntest  = test.fillna(-999)\n\nlabel = \"isFraud\"\nprint(\"Train shape after sampling & cleanup:\", train.shape)\nprint(\"Positive rate:\", train[label].mean().round(4))\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-02T23:32:50.624861Z","iopub.execute_input":"2025-11-02T23:32:50.625208Z","iopub.status.idle":"2025-11-02T23:33:56.596684Z","shell.execute_reply.started":"2025-11-02T23:32:50.625181Z","shell.execute_reply":"2025-11-02T23:33:56.595306Z"}},"outputs":[{"name":"stdout","text":"Loading...\nShapes before sampling: (590540, 434) (506691, 433)\nTrain shape after sampling & cleanup: (59054, 425)\nPositive rate: 0.0357\n","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"# ===== Train (fast preset, CPU only) =====\npredictor = TabularPredictor(label=label, problem_type=\"binary\", path=\"ag_ieee_small\")\n\npredictor.fit(\n    train_data=train,\n    time_limit=480,  # 8 minutes\n    presets=\"medium_quality_faster_train\",  # lighter preset\n    num_bag_folds=0,\n    num_stack_levels=0,\n    verbosity=2\n)\n\nprint(\"\\n=== Leaderboard ===\")\nleader = predictor.leaderboard(silent=True)\nprint(leader.head(10))\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-02T23:34:30.968624Z","iopub.execute_input":"2025-11-02T23:34:30.969005Z","iopub.status.idle":"2025-11-02T23:42:24.416438Z","shell.execute_reply.started":"2025-11-02T23:34:30.968976Z","shell.execute_reply":"2025-11-02T23:42:24.415382Z"}},"outputs":[{"name":"stderr","text":"Preset alias specified: 'medium_quality_faster_train' maps to 'medium_quality'.\nVerbosity: 2 (Standard Logging)\n=================== System Info ===================\nAutoGluon Version:  1.4.0\nPython Version:     3.11.13\nOperating System:   Linux\nPlatform Machine:   x86_64\nPlatform Version:   #1 SMP PREEMPT_DYNAMIC Sun Nov 10 10:07:59 UTC 2024\nCPU Count:          4\nMemory Avail:       23.41 GB / 31.35 GB (74.7%)\nDisk Space Avail:   17.74 GB / 19.52 GB (90.9%)\n===================================================\nPresets specified: ['medium_quality_faster_train']\nUsing hyperparameters preset: hyperparameters='default'\nBeginning AutoGluon training ... Time limit = 480s\nAutoGluon will save models to \"/kaggle/working/ag_ieee_small\"\nTrain Data Rows:    59054\nTrain Data Columns: 424\nLabel Column:       isFraud\nProblem Type:       binary\nPreprocessing data ...\nSelected class <--> label mapping:  class 1 = 1, class 0 = 0\nUsing Feature Generators to preprocess the data ...\nFitting AutoMLPipelineFeatureGenerator...\n\tAvailable Memory:                    24035.22 MB\n\tTrain Data (Original)  Memory Usage: 253.38 MB (1.1% of available memory)\n\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n\tStage 1 Generators:\n\t\tFitting AsTypeFeatureGenerator...\n\tStage 2 Generators:\n\t\tFitting FillNaFeatureGenerator...\n\tStage 3 Generators:\n\t\tFitting IdentityFeatureGenerator...\n\t\tFitting CategoryFeatureGenerator...\n\t\t\tFitting CategoryMemoryMinimizeFeatureGenerator...\n\t\tFitting DatetimeFeatureGenerator...\n/usr/local/lib/python3.11/dist-packages/autogluon/features/generators/datetime.py:70: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  X_datetime[datetime_feature + \".\" + feature] = getattr(\n/usr/local/lib/python3.11/dist-packages/autogluon/features/generators/datetime.py:70: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  X_datetime[datetime_feature + \".\" + feature] = getattr(\n/usr/local/lib/python3.11/dist-packages/autogluon/features/generators/datetime.py:70: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  X_datetime[datetime_feature + \".\" + feature] = getattr(\n/usr/local/lib/python3.11/dist-packages/autogluon/features/generators/datetime.py:70: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  X_datetime[datetime_feature + \".\" + feature] = getattr(\n/usr/local/lib/python3.11/dist-packages/autogluon/features/generators/datetime.py:68: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  X_datetime[datetime_feature] = self.normalize_timeseries(X, datetime_feature, is_fit=is_fit)\n/usr/local/lib/python3.11/dist-packages/autogluon/features/generators/datetime.py:70: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  X_datetime[datetime_feature + \".\" + feature] = getattr(\n/usr/local/lib/python3.11/dist-packages/autogluon/features/generators/datetime.py:70: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  X_datetime[datetime_feature + \".\" + feature] = getattr(\n/usr/local/lib/python3.11/dist-packages/autogluon/features/generators/datetime.py:70: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  X_datetime[datetime_feature + \".\" + feature] = getattr(\n/usr/local/lib/python3.11/dist-packages/autogluon/features/generators/datetime.py:70: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  X_datetime[datetime_feature + \".\" + feature] = getattr(\n/usr/local/lib/python3.11/dist-packages/autogluon/features/generators/datetime.py:68: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  X_datetime[datetime_feature] = self.normalize_timeseries(X, datetime_feature, is_fit=is_fit)\n/usr/local/lib/python3.11/dist-packages/autogluon/features/generators/datetime.py:70: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  X_datetime[datetime_feature + \".\" + feature] = getattr(\n/usr/local/lib/python3.11/dist-packages/autogluon/features/generators/datetime.py:70: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  X_datetime[datetime_feature + \".\" + feature] = getattr(\n/usr/local/lib/python3.11/dist-packages/autogluon/features/generators/datetime.py:70: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  X_datetime[datetime_feature + \".\" + feature] = getattr(\n/usr/local/lib/python3.11/dist-packages/autogluon/features/generators/datetime.py:70: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  X_datetime[datetime_feature + \".\" + feature] = getattr(\n/usr/local/lib/python3.11/dist-packages/autogluon/features/generators/datetime.py:68: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  X_datetime[datetime_feature] = self.normalize_timeseries(X, datetime_feature, is_fit=is_fit)\n/usr/local/lib/python3.11/dist-packages/autogluon/features/generators/datetime.py:70: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  X_datetime[datetime_feature + \".\" + feature] = getattr(\n/usr/local/lib/python3.11/dist-packages/autogluon/features/generators/datetime.py:70: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  X_datetime[datetime_feature + \".\" + feature] = getattr(\n/usr/local/lib/python3.11/dist-packages/autogluon/features/generators/datetime.py:70: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  X_datetime[datetime_feature + \".\" + feature] = getattr(\n/usr/local/lib/python3.11/dist-packages/autogluon/features/generators/datetime.py:70: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  X_datetime[datetime_feature + \".\" + feature] = getattr(\n/usr/local/lib/python3.11/dist-packages/autogluon/features/generators/datetime.py:68: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  X_datetime[datetime_feature] = self.normalize_timeseries(X, datetime_feature, is_fit=is_fit)\n/usr/local/lib/python3.11/dist-packages/autogluon/features/generators/datetime.py:70: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  X_datetime[datetime_feature + \".\" + feature] = getattr(\n/usr/local/lib/python3.11/dist-packages/autogluon/features/generators/datetime.py:70: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  X_datetime[datetime_feature + \".\" + feature] = getattr(\n/usr/local/lib/python3.11/dist-packages/autogluon/features/generators/datetime.py:70: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  X_datetime[datetime_feature + \".\" + feature] = getattr(\n/usr/local/lib/python3.11/dist-packages/autogluon/features/generators/datetime.py:70: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  X_datetime[datetime_feature + \".\" + feature] = getattr(\n\tStage 4 Generators:\n\t\tFitting DropUniqueFeatureGenerator...\n\tStage 5 Generators:\n\t\tFitting DropDuplicatesFeatureGenerator...\n\tUseless Original Features (Count: 1): ['V305']\n\t\tThese features carry no predictive signal and should be manually investigated.\n\t\tThis is typically a feature which has the same value for all rows.\n\t\tThese features do not need to be present at inference time.\n\tUnused Original Features (Count: 2): ['V28', 'V241']\n\t\tThese features were not used to generate any of the output features. Add a feature generator compatible with these features to utilize them.\n\t\tFeatures can also be unused if they carry very little information, such as being categorical but having almost entirely unique values or being duplicates of other features.\n\t\tThese features do not need to be present at inference time.\n\t\t('float', []) : 2 | ['V28', 'V241']\n\tTypes of features in original data (raw dtype, special dtypes):\n\t\t('float', [])                      : 389 | ['TransactionAmt', 'card2', 'card3', 'card5', 'addr1', ...]\n\t\t('int', [])                        :   3 | ['TransactionID', 'TransactionDT', 'card1']\n\t\t('object', [])                     :   4 | ['ProductCD', 'card4', 'card6', 'P_emaildomain']\n\t\t('object', ['datetime_as_object']) :  25 | ['R_emaildomain', 'M1', 'M2', 'M3', 'M4', ...]\n\tTypes of features in processed data (raw dtype, special dtypes):\n\t\t('category', [])             :   4 | ['ProductCD', 'card4', 'card6', 'P_emaildomain']\n\t\t('float', [])                : 389 | ['TransactionAmt', 'card2', 'card3', 'card5', 'addr1', ...]\n\t\t('int', [])                  :   3 | ['TransactionID', 'TransactionDT', 'card1']\n\t\t('int', ['datetime_as_int']) :   5 | ['DeviceInfo', 'DeviceInfo.year', 'DeviceInfo.month', 'DeviceInfo.day', 'DeviceInfo.dayofweek']\n\t5.2s = Fit runtime\n\t421 features in original data used to generate 401 features in processed data.\n\tTrain Data (Processed) Memory Usage: 179.09 MB (0.7% of available memory)\nData preprocessing and feature engineering runtime = 5.62s ...\nAutoGluon will gauge predictive performance using evaluation metric: 'accuracy'\n\tTo change this, specify the eval_metric parameter of Predictor()\nAutomatically generating train/validation split with holdout_frac=0.04233413485962001, Train Rows: 56554, Val Rows: 2500\nUser-specified model hyperparameters to be fit:\n{\n\t'NN_TORCH': [{}],\n\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, {'learning_rate': 0.03, 'num_leaves': 128, 'feature_fraction': 0.9, 'min_data_in_leaf': 3, 'ag_args': {'name_suffix': 'Large', 'priority': 0, 'hyperparameter_tune_kwargs': None}}],\n\t'CAT': [{}],\n\t'XGB': [{}],\n\t'FASTAI': [{}],\n\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n}\nFitting 11 L1 models, fit_strategy=\"sequential\" ...\nFitting model: LightGBMXT ... Training model for up to 474.38s of the 474.38s of remaining time.\n\tFitting with cpus=2, gpus=0, mem=1.0/23.0 GB\n\t0.9724\t = Validation score   (accuracy)\n\t12.55s\t = Training   runtime\n\t0.01s\t = Validation runtime\nFitting model: LightGBM ... Training model for up to 461.81s of the 461.81s of remaining time.\n\tFitting with cpus=2, gpus=0, mem=1.0/22.9 GB\n\t0.974\t = Validation score   (accuracy)\n\t8.13s\t = Training   runtime\n\t0.02s\t = Validation runtime\nFitting model: RandomForestGini ... Training model for up to 453.65s of the 453.64s of remaining time.\n\tFitting with cpus=4, gpus=0, mem=0.0/22.9 GB\n\t0.972\t = Validation score   (accuracy)\n\t41.31s\t = Training   runtime\n\t0.16s\t = Validation runtime\nFitting model: RandomForestEntr ... Training model for up to 412.02s of the 412.02s of remaining time.\n\tFitting with cpus=4, gpus=0, mem=0.0/22.9 GB\n\t0.9716\t = Validation score   (accuracy)\n\t31.41s\t = Training   runtime\n\t0.14s\t = Validation runtime\nFitting model: CatBoost ... Training model for up to 380.27s of the 380.27s of remaining time.\n\tFitting with cpus=2, gpus=0, mem=1.6/22.9 GB\n\t0.9748\t = Validation score   (accuracy)\n\t35.55s\t = Training   runtime\n\t0.03s\t = Validation runtime\nFitting model: ExtraTreesGini ... Training model for up to 344.68s of the 344.68s of remaining time.\n\tFitting with cpus=4, gpus=0, mem=0.0/22.6 GB\n\t0.9696\t = Validation score   (accuracy)\n\t30.96s\t = Training   runtime\n\t0.17s\t = Validation runtime\nFitting model: ExtraTreesEntr ... Training model for up to 313.35s of the 313.35s of remaining time.\n\tFitting with cpus=4, gpus=0, mem=0.0/22.6 GB\n\t0.9696\t = Validation score   (accuracy)\n\t28.76s\t = Training   runtime\n\t0.16s\t = Validation runtime\nFitting model: NeuralNetFastAI ... Training model for up to 284.24s of the 284.24s of remaining time.\n\tFitting with cpus=2, gpus=0, mem=1.7/22.6 GB\n\t0.9724\t = Validation score   (accuracy)\n\t114.46s\t = Training   runtime\n\t0.09s\t = Validation runtime\nFitting model: XGBoost ... Training model for up to 169.62s of the 169.62s of remaining time.\n\tFitting with cpus=2, gpus=0, mem=1.4/22.4 GB\n\t0.9744\t = Validation score   (accuracy)\n\t10.6s\t = Training   runtime\n\t0.07s\t = Validation runtime\nFitting model: NeuralNetTorch ... Training model for up to 158.94s of the 158.94s of remaining time.\n\tFitting with cpus=2, gpus=0, mem=0.8/22.5 GB\n/usr/local/lib/python3.11/dist-packages/sklearn/compose/_column_transformer.py:975: FutureWarning: The parameter `force_int_remainder_cols` is deprecated and will be removed in 1.9. It has no effect. Leave it to its default value to avoid this warning.\n  warnings.warn(\n\t0.9736\t = Validation score   (accuracy)\n\t129.47s\t = Training   runtime\n\t0.32s\t = Validation runtime\nFitting model: LightGBMLarge ... Training model for up to 29.13s of the 29.13s of remaining time.\n\tFitting with cpus=2, gpus=0, mem=1.2/22.5 GB\n\t0.978\t = Validation score   (accuracy)\n\t20.34s\t = Training   runtime\n\t0.05s\t = Validation runtime\nFitting model: WeightedEnsemble_L2 ... Training model for up to 360.00s of the 8.66s of remaining time.\n\tEnsemble Weights: {'LightGBMLarge': 1.0}\n\t0.978\t = Validation score   (accuracy)\n\t0.14s\t = Training   runtime\n\t0.0s\t = Validation runtime\nAutoGluon training complete, total runtime = 471.96s ... Best model: WeightedEnsemble_L2 | Estimated inference throughput: 47770.5 rows/s (2500 batch size)\nDisabling decision threshold calibration for metric `accuracy` due to having fewer than 10000 rows of validation data for calibration, to avoid overfitting (2500 rows).\n\t`accuracy` is generally not improved through threshold calibration. Force calibration via specifying `calibrate_decision_threshold=True`.\nTabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"/kaggle/working/ag_ieee_small\")\n","output_type":"stream"},{"name":"stdout","text":"\n=== Leaderboard ===\n                 model  score_val eval_metric  pred_time_val    fit_time  \\\n0        LightGBMLarge     0.9780    accuracy       0.051179   20.344257   \n1  WeightedEnsemble_L2     0.9780    accuracy       0.052334   20.489179   \n2             CatBoost     0.9748    accuracy       0.027523   35.548538   \n3              XGBoost     0.9744    accuracy       0.073711   10.597123   \n4             LightGBM     0.9740    accuracy       0.019649    8.131025   \n5       NeuralNetTorch     0.9736    accuracy       0.320346  129.467934   \n6           LightGBMXT     0.9724    accuracy       0.014170   12.547744   \n7      NeuralNetFastAI     0.9724    accuracy       0.087279  114.459386   \n8     RandomForestGini     0.9720    accuracy       0.155100   41.313329   \n9     RandomForestEntr     0.9716    accuracy       0.144364   31.411101   \n\n   pred_time_val_marginal  fit_time_marginal  stack_level  can_infer  \\\n0                0.051179          20.344257            1       True   \n1                0.001154           0.144922            2       True   \n2                0.027523          35.548538            1       True   \n3                0.073711          10.597123            1       True   \n4                0.019649           8.131025            1       True   \n5                0.320346         129.467934            1       True   \n6                0.014170          12.547744            1       True   \n7                0.087279         114.459386            1       True   \n8                0.155100          41.313329            1       True   \n9                0.144364          31.411101            1       True   \n\n   fit_order  \n0         11  \n1         12  \n2          5  \n3          9  \n4          2  \n5         10  \n6          1  \n7          8  \n8          3  \n9          4  \n","output_type":"stream"}],"execution_count":3},{"cell_type":"code","source":"# ===== Align test columns and Predict =====\n# works for all AG >= 1.0\n\ntrain_cols = predictor.feature_metadata_in.get_features()  # returns list of training feature names\n\n# ensure the test set matches the same feature space\nmissing = [c for c in train_cols if c not in test.columns]\nfor c in missing:\n    test[c] = -999\nextra = [c for c in test.columns if c not in train_cols + [\"TransactionID\"]]\ntest = test.drop(columns=extra, errors=\"ignore\")\ntest = test[train_cols]  # reorder columns\n\nprint(f\"Aligned: kept {len(train_cols)}, added {len(missing)}, dropped {len(extra)}\")\n\n# predict probabilities for the positive class\npred_proba = predictor.predict_proba(test)[1]\n\n# build submission\nsub = pd.DataFrame({\n    \"TransactionID\": test_id[\"TransactionID\"],\n    \"isFraud\": pred_proba\n})\nsub.to_csv(\"/kaggle/working/submission.csv\", index=False)\nprint(\"✅ Saved: /kaggle/working/submission.csv\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-02T23:48:42.770569Z","iopub.execute_input":"2025-11-02T23:48:42.770929Z","iopub.status.idle":"2025-11-02T23:49:10.157238Z","shell.execute_reply.started":"2025-11-02T23:48:42.770899Z","shell.execute_reply":"2025-11-02T23:49:10.156211Z"}},"outputs":[{"name":"stdout","text":"Aligned: kept 421, added 29, dropped 41\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/autogluon/features/generators/datetime.py:70: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  X_datetime[datetime_feature + \".\" + feature] = getattr(\n/usr/local/lib/python3.11/dist-packages/autogluon/features/generators/datetime.py:70: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  X_datetime[datetime_feature + \".\" + feature] = getattr(\n/usr/local/lib/python3.11/dist-packages/autogluon/features/generators/datetime.py:70: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  X_datetime[datetime_feature + \".\" + feature] = getattr(\n/usr/local/lib/python3.11/dist-packages/autogluon/features/generators/datetime.py:70: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  X_datetime[datetime_feature + \".\" + feature] = getattr(\n/usr/local/lib/python3.11/dist-packages/autogluon/features/generators/datetime.py:68: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  X_datetime[datetime_feature] = self.normalize_timeseries(X, datetime_feature, is_fit=is_fit)\n/usr/local/lib/python3.11/dist-packages/autogluon/features/generators/datetime.py:70: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  X_datetime[datetime_feature + \".\" + feature] = getattr(\n/usr/local/lib/python3.11/dist-packages/autogluon/features/generators/datetime.py:70: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  X_datetime[datetime_feature + \".\" + feature] = getattr(\n/usr/local/lib/python3.11/dist-packages/autogluon/features/generators/datetime.py:70: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  X_datetime[datetime_feature + \".\" + feature] = getattr(\n/usr/local/lib/python3.11/dist-packages/autogluon/features/generators/datetime.py:70: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  X_datetime[datetime_feature + \".\" + feature] = getattr(\n/usr/local/lib/python3.11/dist-packages/autogluon/features/generators/datetime.py:68: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  X_datetime[datetime_feature] = self.normalize_timeseries(X, datetime_feature, is_fit=is_fit)\n/usr/local/lib/python3.11/dist-packages/autogluon/features/generators/datetime.py:70: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  X_datetime[datetime_feature + \".\" + feature] = getattr(\n/usr/local/lib/python3.11/dist-packages/autogluon/features/generators/datetime.py:70: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  X_datetime[datetime_feature + \".\" + feature] = getattr(\n/usr/local/lib/python3.11/dist-packages/autogluon/features/generators/datetime.py:70: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  X_datetime[datetime_feature + \".\" + feature] = getattr(\n/usr/local/lib/python3.11/dist-packages/autogluon/features/generators/datetime.py:70: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  X_datetime[datetime_feature + \".\" + feature] = getattr(\n/usr/local/lib/python3.11/dist-packages/autogluon/features/generators/datetime.py:68: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  X_datetime[datetime_feature] = self.normalize_timeseries(X, datetime_feature, is_fit=is_fit)\n/usr/local/lib/python3.11/dist-packages/autogluon/features/generators/datetime.py:70: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  X_datetime[datetime_feature + \".\" + feature] = getattr(\n/usr/local/lib/python3.11/dist-packages/autogluon/features/generators/datetime.py:70: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  X_datetime[datetime_feature + \".\" + feature] = getattr(\n/usr/local/lib/python3.11/dist-packages/autogluon/features/generators/datetime.py:70: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  X_datetime[datetime_feature + \".\" + feature] = getattr(\n/usr/local/lib/python3.11/dist-packages/autogluon/features/generators/datetime.py:70: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  X_datetime[datetime_feature + \".\" + feature] = getattr(\n/usr/local/lib/python3.11/dist-packages/autogluon/features/generators/datetime.py:68: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  X_datetime[datetime_feature] = self.normalize_timeseries(X, datetime_feature, is_fit=is_fit)\n/usr/local/lib/python3.11/dist-packages/autogluon/features/generators/datetime.py:70: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  X_datetime[datetime_feature + \".\" + feature] = getattr(\n/usr/local/lib/python3.11/dist-packages/autogluon/features/generators/datetime.py:70: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  X_datetime[datetime_feature + \".\" + feature] = getattr(\n/usr/local/lib/python3.11/dist-packages/autogluon/features/generators/datetime.py:70: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  X_datetime[datetime_feature + \".\" + feature] = getattr(\n/usr/local/lib/python3.11/dist-packages/autogluon/features/generators/datetime.py:70: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  X_datetime[datetime_feature + \".\" + feature] = getattr(\n","output_type":"stream"},{"name":"stdout","text":"✅ Saved: /kaggle/working/submission.csv\n","output_type":"stream"}],"execution_count":7},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}