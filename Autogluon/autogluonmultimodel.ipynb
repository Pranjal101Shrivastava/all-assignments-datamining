{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":482,"sourceType":"datasetVersion","datasetId":228}],"dockerImageVersionId":31153,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# 1) Install\n!pip -q install -U autogluon.tabular==1.4.0\n\n# 2) Load\nimport pandas as pd\nfrom autogluon.tabular import TabularPredictor\n\ndf = pd.read_csv(\"/kaggle/input/pima-indians-diabetes-database/diabetes.csv\")\nlabel = \"Outcome\"\nprint(df.shape, df[label].mean())  # pos rate\n\n# 3) Train many models (LightGBM, XGBoost, CatBoost, RF, ExtraTrees)\npredictor = TabularPredictor(label=label, problem_type=\"binary\", path=\"ag_pima_multi\")\npredictor.fit(\n    train_data=df,\n    time_limit=180,                 # ~3 min cap\n    presets=\"medium_quality\",\n    hyperparameters={\n        \"GBM\": [{}],               # LightGBM\n        \"XGB\": [{}],               # XGBoost\n        \"CAT\": [{}],               # CatBoost (CPU ok)\n        \"RF\":  [{}],               # RandomForest\n        \"XT\":  [{}],               # ExtraTrees\n        \"KNN\": [], \"NN_TORCH\": []  # skip slower ones\n    },\n    verbosity=2,\n)\n\n# 4) Leaderboard\nleader = predictor.leaderboard(silent=True)\nprint(leader.head(10))\n\n# 5) Predict on the same data (demo) and save\npreds = predictor.predict(df.drop(columns=[label]))\nproba = predictor.predict_proba(df.drop(columns=[label]))[1]\nout = pd.DataFrame({\"pred\": preds, \"proba_1\": proba})\nout.to_csv(\"/kaggle/working/pima_preds.csv\", index=False)\nprint(\"Saved /kaggle/working/pima_preds.csv\")\n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-11-02T23:59:14.063261Z","iopub.execute_input":"2025-11-02T23:59:14.063523Z","iopub.status.idle":"2025-11-02T23:59:46.094630Z","shell.execute_reply.started":"2025-11-02T23:59:14.063490Z","shell.execute_reply":"2025-11-02T23:59:46.093608Z"}},"outputs":[{"name":"stdout","text":"\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m487.3/487.3 kB\u001b[0m \u001b[31m20.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m225.1/225.1 kB\u001b[0m \u001b[31m12.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m64.2/64.2 kB\u001b[0m \u001b[31m4.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m71.0/71.0 kB\u001b[0m \u001b[31m4.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m9.7/9.7 MB\u001b[0m \u001b[31m21.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0mm\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m278.0/278.0 kB\u001b[0m \u001b[31m18.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\ncategory-encoders 2.7.0 requires scikit-learn<1.6.0,>=1.0.0, but you have scikit-learn 1.7.2 which is incompatible.\ncesium 0.12.4 requires numpy<3.0,>=2.0, but you have numpy 1.26.4 which is incompatible.\nsklearn-compat 0.1.3 requires scikit-learn<1.7,>=1.2, but you have scikit-learn 1.7.2 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0m","output_type":"stream"},{"name":"stderr","text":"Verbosity: 2 (Standard Logging)\n=================== System Info ===================\nAutoGluon Version:  1.4.0\nPython Version:     3.11.13\nOperating System:   Linux\nPlatform Machine:   x86_64\nPlatform Version:   #1 SMP PREEMPT_DYNAMIC Sun Nov 10 10:07:59 UTC 2024\nCPU Count:          4\nMemory Avail:       30.02 GB / 31.35 GB (95.8%)\nDisk Space Avail:   19.50 GB / 19.52 GB (99.9%)\n===================================================\nPresets specified: ['medium_quality']\n","output_type":"stream"},{"name":"stdout","text":"(768, 9) 0.3489583333333333\n","output_type":"stream"},{"name":"stderr","text":"Beginning AutoGluon training ... Time limit = 180s\nAutoGluon will save models to \"/kaggle/working/ag_pima_multi\"\nTrain Data Rows:    768\nTrain Data Columns: 8\nLabel Column:       Outcome\nProblem Type:       binary\nPreprocessing data ...\nSelected class <--> label mapping:  class 1 = 1, class 0 = 0\nUsing Feature Generators to preprocess the data ...\nFitting AutoMLPipelineFeatureGenerator...\n\tAvailable Memory:                    30740.23 MB\n\tTrain Data (Original)  Memory Usage: 0.05 MB (0.0% of available memory)\n\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n\tStage 1 Generators:\n\t\tFitting AsTypeFeatureGenerator...\n\tStage 2 Generators:\n\t\tFitting FillNaFeatureGenerator...\n\tStage 3 Generators:\n\t\tFitting IdentityFeatureGenerator...\n\tStage 4 Generators:\n\t\tFitting DropUniqueFeatureGenerator...\n\tStage 5 Generators:\n\t\tFitting DropDuplicatesFeatureGenerator...\n\tTypes of features in original data (raw dtype, special dtypes):\n\t\t('float', []) : 2 | ['BMI', 'DiabetesPedigreeFunction']\n\t\t('int', [])   : 6 | ['Pregnancies', 'Glucose', 'BloodPressure', 'SkinThickness', 'Insulin', ...]\n\tTypes of features in processed data (raw dtype, special dtypes):\n\t\t('float', []) : 2 | ['BMI', 'DiabetesPedigreeFunction']\n\t\t('int', [])   : 6 | ['Pregnancies', 'Glucose', 'BloodPressure', 'SkinThickness', 'Insulin', ...]\n\t0.0s = Fit runtime\n\t8 features in original data used to generate 8 features in processed data.\n\tTrain Data (Processed) Memory Usage: 0.05 MB (0.0% of available memory)\nData preprocessing and feature engineering runtime = 0.06s ...\nAutoGluon will gauge predictive performance using evaluation metric: 'accuracy'\n\tTo change this, specify the eval_metric parameter of Predictor()\nAutomatically generating train/validation split with holdout_frac=0.2, Train Rows: 614, Val Rows: 154\nUser-specified model hyperparameters to be fit:\n{\n\t'GBM': [{}],\n\t'XGB': [{}],\n\t'CAT': [{}],\n\t'RF': [{}],\n\t'XT': [{}],\n\t'KNN': [],\n\t'NN_TORCH': [],\n}\nFitting 5 L1 models, fit_strategy=\"sequential\" ...\nFitting model: LightGBM ... Training model for up to 179.94s of the 179.94s of remaining time.\n\tFitting with cpus=2, gpus=0, mem=0.0/29.8 GB\n\t0.8117\t = Validation score   (accuracy)\n\t9.51s\t = Training   runtime\n\t0.0s\t = Validation runtime\nFitting model: RandomForest ... Training model for up to 170.41s of the 170.41s of remaining time.\n\tFitting with cpus=4, gpus=0\n\t0.8182\t = Validation score   (accuracy)\n\t0.92s\t = Training   runtime\n\t0.09s\t = Validation runtime\nFitting model: CatBoost ... Training model for up to 169.38s of the 169.37s of remaining time.\n\tFitting with cpus=2, gpus=0\n\t0.8247\t = Validation score   (accuracy)\n\t1.14s\t = Training   runtime\n\t0.0s\t = Validation runtime\nFitting model: ExtraTrees ... Training model for up to 168.22s of the 168.22s of remaining time.\n\tFitting with cpus=4, gpus=0\n\t0.7987\t = Validation score   (accuracy)\n\t0.87s\t = Training   runtime\n\t0.08s\t = Validation runtime\nFitting model: XGBoost ... Training model for up to 167.24s of the 167.24s of remaining time.\n\tFitting with cpus=2, gpus=0\n\t0.8117\t = Validation score   (accuracy)\n\t0.35s\t = Training   runtime\n\t0.0s\t = Validation runtime\nFitting model: WeightedEnsemble_L2 ... Training model for up to 179.94s of the 166.87s of remaining time.\n\tEnsemble Weights: {'CatBoost': 1.0}\n\t0.8247\t = Validation score   (accuracy)\n\t0.06s\t = Training   runtime\n\t0.0s\t = Validation runtime\nAutoGluon training complete, total runtime = 13.22s ... Best model: WeightedEnsemble_L2 | Estimated inference throughput: 51455.7 rows/s (154 batch size)\nDisabling decision threshold calibration for metric `accuracy` due to having fewer than 10000 rows of validation data for calibration, to avoid overfitting (154 rows).\n\t`accuracy` is generally not improved through threshold calibration. Force calibration via specifying `calibrate_decision_threshold=True`.\nTabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"/kaggle/working/ag_pima_multi\")\n","output_type":"stream"},{"name":"stdout","text":"                 model  score_val eval_metric  pred_time_val  fit_time  \\\n0             CatBoost   0.824675    accuracy       0.002093  1.143446   \n1  WeightedEnsemble_L2   0.824675    accuracy       0.002993  1.200856   \n2         RandomForest   0.818182    accuracy       0.089082  0.923418   \n3             LightGBM   0.811688    accuracy       0.001062  9.514475   \n4              XGBoost   0.811688    accuracy       0.003733  0.346567   \n5           ExtraTrees   0.798701    accuracy       0.078637  0.873075   \n\n   pred_time_val_marginal  fit_time_marginal  stack_level  can_infer  \\\n0                0.002093           1.143446            1       True   \n1                0.000900           0.057410            2       True   \n2                0.089082           0.923418            1       True   \n3                0.001062           9.514475            1       True   \n4                0.003733           0.346567            1       True   \n5                0.078637           0.873075            1       True   \n\n   fit_order  \n0          3  \n1          6  \n2          2  \n3          1  \n4          5  \n5          4  \nSaved /kaggle/working/pima_preds.csv\n","output_type":"stream"}],"execution_count":1}]}