{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":477177,"sourceType":"datasetVersion","datasetId":216167}],"dockerImageVersionId":31153,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# ==== 1. Install ====\n!pip -q install -U autogluon.tabular==1.4.0\n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-11-03T00:02:33.445202Z","iopub.execute_input":"2025-11-03T00:02:33.445539Z","iopub.status.idle":"2025-11-03T00:02:50.887705Z","shell.execute_reply.started":"2025-11-03T00:02:33.445508Z","shell.execute_reply":"2025-11-03T00:02:50.886513Z"}},"outputs":[{"name":"stdout","text":"\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m487.3/487.3 kB\u001b[0m \u001b[31m8.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mta \u001b[36m0:00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m225.1/225.1 kB\u001b[0m \u001b[31m11.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m64.2/64.2 kB\u001b[0m \u001b[31m3.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m71.0/71.0 kB\u001b[0m \u001b[31m2.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m9.7/9.7 MB\u001b[0m \u001b[31m66.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m278.0/278.0 kB\u001b[0m \u001b[31m12.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\ncategory-encoders 2.7.0 requires scikit-learn<1.6.0,>=1.0.0, but you have scikit-learn 1.7.2 which is incompatible.\ncesium 0.12.4 requires numpy<3.0,>=2.0, but you have numpy 1.26.4 which is incompatible.\nsklearn-compat 0.1.3 requires scikit-learn<1.7,>=1.2, but you have scikit-learn 1.7.2 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0m","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"# ==== 2. Load dataset ====\nimport pandas as pd\nfrom autogluon.tabular import TabularPredictor\n\ndf = pd.read_csv(\"/kaggle/input/heart-disease-dataset/heart.csv\")\nlabel = \"target\"\nprint(df.shape)\ndf.head()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-03T00:03:19.409429Z","iopub.execute_input":"2025-11-03T00:03:19.409751Z","iopub.status.idle":"2025-11-03T00:03:21.218021Z","shell.execute_reply.started":"2025-11-03T00:03:19.409722Z","shell.execute_reply":"2025-11-03T00:03:21.216990Z"}},"outputs":[{"name":"stdout","text":"(1025, 14)\n","output_type":"stream"},{"execution_count":2,"output_type":"execute_result","data":{"text/plain":"   age  sex  cp  trestbps  chol  fbs  restecg  thalach  exang  oldpeak  slope  \\\n0   52    1   0       125   212    0        1      168      0      1.0      2   \n1   53    1   0       140   203    1        0      155      1      3.1      0   \n2   70    1   0       145   174    0        1      125      1      2.6      0   \n3   61    1   0       148   203    0        1      161      0      0.0      2   \n4   62    0   0       138   294    1        1      106      0      1.9      1   \n\n   ca  thal  target  \n0   2     3       0  \n1   0     3       0  \n2   0     3       0  \n3   1     3       0  \n4   3     2       0  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>age</th>\n      <th>sex</th>\n      <th>cp</th>\n      <th>trestbps</th>\n      <th>chol</th>\n      <th>fbs</th>\n      <th>restecg</th>\n      <th>thalach</th>\n      <th>exang</th>\n      <th>oldpeak</th>\n      <th>slope</th>\n      <th>ca</th>\n      <th>thal</th>\n      <th>target</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>52</td>\n      <td>1</td>\n      <td>0</td>\n      <td>125</td>\n      <td>212</td>\n      <td>0</td>\n      <td>1</td>\n      <td>168</td>\n      <td>0</td>\n      <td>1.0</td>\n      <td>2</td>\n      <td>2</td>\n      <td>3</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>53</td>\n      <td>1</td>\n      <td>0</td>\n      <td>140</td>\n      <td>203</td>\n      <td>1</td>\n      <td>0</td>\n      <td>155</td>\n      <td>1</td>\n      <td>3.1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>3</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>70</td>\n      <td>1</td>\n      <td>0</td>\n      <td>145</td>\n      <td>174</td>\n      <td>0</td>\n      <td>1</td>\n      <td>125</td>\n      <td>1</td>\n      <td>2.6</td>\n      <td>0</td>\n      <td>0</td>\n      <td>3</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>61</td>\n      <td>1</td>\n      <td>0</td>\n      <td>148</td>\n      <td>203</td>\n      <td>0</td>\n      <td>1</td>\n      <td>161</td>\n      <td>0</td>\n      <td>0.0</td>\n      <td>2</td>\n      <td>1</td>\n      <td>3</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>62</td>\n      <td>0</td>\n      <td>0</td>\n      <td>138</td>\n      <td>294</td>\n      <td>1</td>\n      <td>1</td>\n      <td>106</td>\n      <td>0</td>\n      <td>1.9</td>\n      <td>1</td>\n      <td>3</td>\n      <td>2</td>\n      <td>0</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}],"execution_count":2},{"cell_type":"code","source":"# ==== 3. Add a few manual features (for demo) ====\ndf[\"chol_to_age\"] = df[\"chol\"] / (df[\"age\"] + 1)\ndf[\"bp_diff\"] = df[\"trestbps\"] - df[\"oldpeak\"]\ndf[\"is_senior\"] = (df[\"age\"] > 60).astype(int)\nprint(\"Added feature columns:\", [c for c in df.columns if c not in [\"age\",\"chol\",\"trestbps\",\"oldpeak\",\"target\"]])\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-03T00:03:38.979282Z","iopub.execute_input":"2025-11-03T00:03:38.979641Z","iopub.status.idle":"2025-11-03T00:03:38.991645Z","shell.execute_reply.started":"2025-11-03T00:03:38.979616Z","shell.execute_reply":"2025-11-03T00:03:38.990558Z"}},"outputs":[{"name":"stdout","text":"Added feature columns: ['sex', 'cp', 'fbs', 'restecg', 'thalach', 'exang', 'slope', 'ca', 'thal', 'chol_to_age', 'bp_diff', 'is_senior']\n","output_type":"stream"}],"execution_count":3},{"cell_type":"code","source":"# ==== 4. AutoGluon setup + Auto Feature Engineering ====\npredictor = TabularPredictor(label=label, problem_type=\"binary\", path=\"ag_heart_fe\")\n\npredictor.fit(\n    train_data=df,\n    time_limit=120,   # 2 minutes\n    presets=\"medium_quality\",\n    verbosity=2,\n)\nleader = predictor.leaderboard(silent=True)\nprint(leader.head(10))\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-03T00:03:49.044738Z","iopub.execute_input":"2025-11-03T00:03:49.045181Z","iopub.status.idle":"2025-11-03T00:04:25.838251Z","shell.execute_reply.started":"2025-11-03T00:03:49.045157Z","shell.execute_reply":"2025-11-03T00:04:25.837294Z"}},"outputs":[{"name":"stderr","text":"Verbosity: 2 (Standard Logging)\n=================== System Info ===================\nAutoGluon Version:  1.4.0\nPython Version:     3.11.13\nOperating System:   Linux\nPlatform Machine:   x86_64\nPlatform Version:   #1 SMP PREEMPT_DYNAMIC Sun Nov 10 10:07:59 UTC 2024\nCPU Count:          4\nMemory Avail:       29.88 GB / 31.35 GB (95.3%)\nDisk Space Avail:   19.50 GB / 19.52 GB (99.9%)\n===================================================\nPresets specified: ['medium_quality']\nUsing hyperparameters preset: hyperparameters='default'\nBeginning AutoGluon training ... Time limit = 120s\nAutoGluon will save models to \"/kaggle/working/ag_heart_fe\"\nTrain Data Rows:    1025\nTrain Data Columns: 16\nLabel Column:       target\nProblem Type:       binary\nPreprocessing data ...\nSelected class <--> label mapping:  class 1 = 1, class 0 = 0\nUsing Feature Generators to preprocess the data ...\nFitting AutoMLPipelineFeatureGenerator...\n\tAvailable Memory:                    30598.58 MB\n\tTrain Data (Original)  Memory Usage: 0.13 MB (0.0% of available memory)\n\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n\tStage 1 Generators:\n\t\tFitting AsTypeFeatureGenerator...\n\t\t\tNote: Converting 4 features to boolean dtype as they only contain 2 unique values.\n\tStage 2 Generators:\n\t\tFitting FillNaFeatureGenerator...\n\tStage 3 Generators:\n\t\tFitting IdentityFeatureGenerator...\n\tStage 4 Generators:\n\t\tFitting DropUniqueFeatureGenerator...\n\tStage 5 Generators:\n\t\tFitting DropDuplicatesFeatureGenerator...\n\tTypes of features in original data (raw dtype, special dtypes):\n\t\t('float', []) :  3 | ['oldpeak', 'chol_to_age', 'bp_diff']\n\t\t('int', [])   : 13 | ['age', 'sex', 'cp', 'trestbps', 'chol', ...]\n\tTypes of features in processed data (raw dtype, special dtypes):\n\t\t('float', [])     : 3 | ['oldpeak', 'chol_to_age', 'bp_diff']\n\t\t('int', [])       : 9 | ['age', 'cp', 'trestbps', 'chol', 'restecg', ...]\n\t\t('int', ['bool']) : 4 | ['sex', 'fbs', 'exang', 'is_senior']\n\t0.1s = Fit runtime\n\t16 features in original data used to generate 16 features in processed data.\n\tTrain Data (Processed) Memory Usage: 0.10 MB (0.0% of available memory)\nData preprocessing and feature engineering runtime = 0.1s ...\nAutoGluon will gauge predictive performance using evaluation metric: 'accuracy'\n\tTo change this, specify the eval_metric parameter of Predictor()\nAutomatically generating train/validation split with holdout_frac=0.2, Train Rows: 820, Val Rows: 205\nUser-specified model hyperparameters to be fit:\n{\n\t'NN_TORCH': [{}],\n\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, {'learning_rate': 0.03, 'num_leaves': 128, 'feature_fraction': 0.9, 'min_data_in_leaf': 3, 'ag_args': {'name_suffix': 'Large', 'priority': 0, 'hyperparameter_tune_kwargs': None}}],\n\t'CAT': [{}],\n\t'XGB': [{}],\n\t'FASTAI': [{}],\n\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n}\nFitting 11 L1 models, fit_strategy=\"sequential\" ...\nFitting model: LightGBMXT ... Training model for up to 119.90s of the 119.90s of remaining time.\n\tFitting with cpus=2, gpus=0, mem=0.0/29.7 GB\n\t1.0\t = Validation score   (accuracy)\n\t10.68s\t = Training   runtime\n\t0.0s\t = Validation runtime\nFitting model: LightGBM ... Training model for up to 109.20s of the 109.20s of remaining time.\n\tFitting with cpus=2, gpus=0, mem=0.0/29.6 GB\n\t1.0\t = Validation score   (accuracy)\n\t0.5s\t = Training   runtime\n\t0.0s\t = Validation runtime\nFitting model: RandomForestGini ... Training model for up to 108.68s of the 108.68s of remaining time.\n\tFitting with cpus=4, gpus=0\n\t1.0\t = Validation score   (accuracy)\n\t1.06s\t = Training   runtime\n\t0.1s\t = Validation runtime\nFitting model: RandomForestEntr ... Training model for up to 107.51s of the 107.51s of remaining time.\n\tFitting with cpus=4, gpus=0\n\t1.0\t = Validation score   (accuracy)\n\t0.97s\t = Training   runtime\n\t0.09s\t = Validation runtime\nFitting model: CatBoost ... Training model for up to 106.43s of the 106.43s of remaining time.\n\tFitting with cpus=2, gpus=0\n\t1.0\t = Validation score   (accuracy)\n\t1.31s\t = Training   runtime\n\t0.0s\t = Validation runtime\nFitting model: ExtraTreesGini ... Training model for up to 105.11s of the 105.10s of remaining time.\n\tFitting with cpus=4, gpus=0\n\t1.0\t = Validation score   (accuracy)\n\t0.98s\t = Training   runtime\n\t0.1s\t = Validation runtime\nFitting model: ExtraTreesEntr ... Training model for up to 103.99s of the 103.99s of remaining time.\n\tFitting with cpus=4, gpus=0\n\t1.0\t = Validation score   (accuracy)\n\t1.02s\t = Training   runtime\n\t0.1s\t = Validation runtime\nFitting model: NeuralNetFastAI ... Training model for up to 102.85s of the 102.85s of remaining time.\n\tFitting with cpus=2, gpus=0, mem=0.0/29.6 GB\n\t0.9854\t = Validation score   (accuracy)\n\t2.96s\t = Training   runtime\n\t0.02s\t = Validation runtime\nFitting model: XGBoost ... Training model for up to 99.85s of the 99.85s of remaining time.\n\tFitting with cpus=2, gpus=0\n\t1.0\t = Validation score   (accuracy)\n\t0.27s\t = Training   runtime\n\t0.0s\t = Validation runtime\nFitting model: NeuralNetTorch ... Training model for up to 99.57s of the 99.56s of remaining time.\n\tFitting with cpus=2, gpus=0, mem=0.0/29.6 GB\n/usr/local/lib/python3.11/dist-packages/sklearn/compose/_column_transformer.py:975: FutureWarning: The parameter `force_int_remainder_cols` is deprecated and will be removed in 1.9. It has no effect. Leave it to its default value to avoid this warning.\n  warnings.warn(\n\t1.0\t = Validation score   (accuracy)\n\t13.74s\t = Training   runtime\n\t0.01s\t = Validation runtime\nFitting model: LightGBMLarge ... Training model for up to 85.81s of the 85.81s of remaining time.\n\tFitting with cpus=2, gpus=0, mem=0.0/29.5 GB\n\t1.0\t = Validation score   (accuracy)\n\t1.09s\t = Training   runtime\n\t0.0s\t = Validation runtime\nFitting model: WeightedEnsemble_L2 ... Training model for up to 119.90s of the 84.70s of remaining time.\n\tEnsemble Weights: {'XGBoost': 1.0}\n\t1.0\t = Validation score   (accuracy)\n\t0.08s\t = Training   runtime\n\t0.0s\t = Validation runtime\nAutoGluon training complete, total runtime = 35.41s ... Best model: WeightedEnsemble_L2 | Estimated inference throughput: 43101.5 rows/s (205 batch size)\nDisabling decision threshold calibration for metric `accuracy` due to having fewer than 10000 rows of validation data for calibration, to avoid overfitting (205 rows).\n\t`accuracy` is generally not improved through threshold calibration. Force calibration via specifying `calibrate_decision_threshold=True`.\nTabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"/kaggle/working/ag_heart_fe\")\n","output_type":"stream"},{"name":"stdout","text":"                 model  score_val eval_metric  pred_time_val   fit_time  \\\n0        LightGBMLarge        1.0    accuracy       0.001347   1.093103   \n1             LightGBM        1.0    accuracy       0.001791   0.504736   \n2           LightGBMXT        1.0    accuracy       0.003227  10.682480   \n3             CatBoost        1.0    accuracy       0.003500   1.311315   \n4              XGBoost        1.0    accuracy       0.003799   0.271341   \n5  WeightedEnsemble_L2        1.0    accuracy       0.004756   0.353500   \n6       NeuralNetTorch        1.0    accuracy       0.008717  13.735564   \n7     RandomForestEntr        1.0    accuracy       0.088199   0.971641   \n8     RandomForestGini        1.0    accuracy       0.099861   1.056627   \n9       ExtraTreesEntr        1.0    accuracy       0.101648   1.019702   \n\n   pred_time_val_marginal  fit_time_marginal  stack_level  can_infer  \\\n0                0.001347           1.093103            1       True   \n1                0.001791           0.504736            1       True   \n2                0.003227          10.682480            1       True   \n3                0.003500           1.311315            1       True   \n4                0.003799           0.271341            1       True   \n5                0.000957           0.082159            2       True   \n6                0.008717          13.735564            1       True   \n7                0.088199           0.971641            1       True   \n8                0.099861           1.056627            1       True   \n9                0.101648           1.019702            1       True   \n\n   fit_order  \n0         11  \n1          2  \n2          1  \n3          5  \n4          9  \n5         12  \n6         10  \n7          4  \n8          3  \n9          7  \n","output_type":"stream"}],"execution_count":4},{"cell_type":"code","source":"from autogluon.features import AutoMLPipelineFeatureGenerator\n\nX = df.drop(columns=[label])\ngen = AutoMLPipelineFeatureGenerator()\n_ = gen.fit_transform(X)\n\n# Version-robust access\nfm = getattr(gen, \"feature_metadata_out\", None) or getattr(gen, \"feature_metadata_out_\", None)\nif fm is not None:\n    try:\n        print(fm.type_map_raw)\n    except AttributeError:\n        print(fm.get_type_map_raw())\nelse:\n    # Fallback: just show dtypes of engineered features\n    print(\"No feature_metadata_out on this version; showing dtypes instead:\")\n    print(_.dtypes.value_counts())\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-03T00:10:45.015169Z","iopub.execute_input":"2025-11-03T00:10:45.015517Z","iopub.status.idle":"2025-11-03T00:10:45.088174Z","shell.execute_reply.started":"2025-11-03T00:10:45.015493Z","shell.execute_reply":"2025-11-03T00:10:45.087320Z"}},"outputs":[{"name":"stderr","text":"Fitting AutoMLPipelineFeatureGenerator...\n\tAvailable Memory:                    30571.75 MB\n\tTrain Data (Original)  Memory Usage: 0.13 MB (0.0% of available memory)\n\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n\tStage 1 Generators:\n\t\tFitting AsTypeFeatureGenerator...\n\t\t\tNote: Converting 4 features to boolean dtype as they only contain 2 unique values.\n\tStage 2 Generators:\n\t\tFitting FillNaFeatureGenerator...\n\tStage 3 Generators:\n\t\tFitting IdentityFeatureGenerator...\n\tStage 4 Generators:\n\t\tFitting DropUniqueFeatureGenerator...\n\tStage 5 Generators:\n\t\tFitting DropDuplicatesFeatureGenerator...\n\tTypes of features in original data (raw dtype, special dtypes):\n\t\t('float', []) :  3 | ['oldpeak', 'chol_to_age', 'bp_diff']\n\t\t('int', [])   : 13 | ['age', 'sex', 'cp', 'trestbps', 'chol', ...]\n\tTypes of features in processed data (raw dtype, special dtypes):\n\t\t('float', [])     : 3 | ['oldpeak', 'chol_to_age', 'bp_diff']\n\t\t('int', [])       : 9 | ['age', 'cp', 'trestbps', 'chol', 'restecg', ...]\n\t\t('int', ['bool']) : 4 | ['sex', 'fbs', 'exang', 'is_senior']\n\t0.1s = Fit runtime\n\t16 features in original data used to generate 16 features in processed data.\n\tTrain Data (Processed) Memory Usage: 0.10 MB (0.0% of available memory)\n","output_type":"stream"},{"name":"stdout","text":"No feature_metadata_out on this version; showing dtypes instead:\nint64      9\nint8       4\nfloat64    3\nName: count, dtype: int64\n","output_type":"stream"}],"execution_count":7},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}